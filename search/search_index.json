{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"1. About","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"#lab-overview","title":"Lab Overview","text":"<p>During this hands-on training, we\u2019ll learn how to capture logs from Kubernetes using OpenTelemetry and ship them to Dynatrace for analysis.  This will demonstrate how to use Dynatrace with OpenTelemetry; without any Dynatrace native components installed on the Kubernetes cluster (Operator, OneAgent, ActiveGate, etc.).  We'll then utilize Dynatrace OpenPipeline to process OpenTelemetry logs at ingest, to manipulate fields, extract metrics, raise alert events, and manage retention periods, in order to make them easier to analyze and leverage.</p> <p>Lab tasks:</p> <ol> <li>Ingest Kubernetes logs using OpenTelemetry Collector</li> <li>Deploy OpenTelemetry Collector for logs, traces, and metrics</li> <li>Create custom Buckets for Grail storage management</li> <li>Process Astronomy Shop logs with Dynatrace OpenPipeline</li> <li>Process Kubernetes Events logs with Dynatrace OpenPipeline</li> <li>Process OpenTelemetry Collector logs with Dynatrace OpenPipeline</li> <li>Query and visualize logs and metrics in Dynatrace using DQL</li> </ol>"},{"location":"#technical-specification","title":"Technical Specification","text":""},{"location":"#technologies-used","title":"Technologies Used","text":"<ul> <li>Dynatrace</li> <li>Kubernetes Kind<ul> <li>tested on Kind tag 0.27.0</li> </ul> </li> <li>Cert Manager - *prerequisite for OpenTelemetry Operator<ul> <li>tested on cert-manager v1.14.4</li> </ul> </li> <li>OpenTelemetry Operator<ul> <li>tested on v0.103.0 (June 2024)</li> </ul> </li> <li>OpenTelemetry Collector - Dynatrace Distro<ul> <li>tested on v0.25.0 (March 2025)</li> </ul> </li> <li>OpenTelemetry Collector - Contrib Distro<ul> <li>tested on v0.121.0 (March 2025)</li> </ul> </li> <li>OpenTelemetry AstronomyShop Helm Chart<ul> <li>tested on v0.31.0 (June 2024)</li> </ul> </li> </ul>"},{"location":"#reference-architecture","title":"Reference Architecture","text":"<p>OpenTelemetry Astronomy Shop Demo Architecture</p>"},{"location":"#continue","title":"Continue","text":"<ul> <li>Continue to Getting Started (Prerequisites)</li> </ul>"},{"location":"2-getting-started/","title":"2. Getting started","text":"<p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"2-getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Identify Dynatrace OTLP Endpoint</li> <li>Generate Dynatrace Access Token</li> <li>Identify Your Initials</li> <li>Add Bucket Storage Management Permissions</li> </ul>"},{"location":"2-getting-started/#identify-dynatrace-otlp-endpoint","title":"Identify Dynatrace OTLP Endpoint","text":"<p>The OpenTelemetry Protocol (OTLP) is the principal network protocol for the exchange of telemetry data between OpenTelemetry-backed services and applications.  The Dynatrace SaaS tenant provides an OTLP endpoint.</p> <p>See Related Export with OTLP Documentation</p> <p>Identify and save/store your OTLP endpoint for the Dynatrace SaaS tenant:</p> <p>No Trailing Slash</p> <p>Do not include a trailing slash!</p> Type URL Pattern Live (Prod) https://{your-environment-id}.live.dynatrace.com/api/v2/otlp Stage https://{your-environment-id}.sprint.dynatracelabs.com/api/v2/otlp ActiveGate https://{your-activegate-domain}:9999/e/{your-environment-id}/api/v2/otlp"},{"location":"2-getting-started/#generate-dynatrace-access-token","title":"Generate Dynatrace Access Token","text":"<p>Generate a new API access token with the following scopes: <pre><code>Ingest events\nIngest logs\nIngest metrics\nIngest OpenTelemetry traces\n</code></pre> See Related Dynatrace API Token Creation Documentation</p> <p></p>"},{"location":"2-getting-started/#identify-your-initials","title":"Identify Your Initials","text":"<p>In this lab, we'll uniquely identify your OpenTelemetry data using your initials; in case you are using a shared tenant.  We'll be using <code>&lt;INITIALS&gt;-k8s-otel-o11y</code> as our pattern.  Identify your initials (3-5 characters) and use them whenever prompted during the lab.</p>"},{"location":"2-getting-started/#add-storage-management-permissions","title":"Add Storage Management Permissions","text":"<p>The Grail data model consists of buckets, tables, and views.  Records are stored in buckets.  Buckets are assigned to tables, including logs, metrics, events, security events, and bizevents tables. Fetching from a table returns all records from all buckets that are assigned to that table.  To manage your buckets, ensure that you have configured the following permissions:</p> <ul> <li>storage:bucket-definitions:read</li> <li>storage:bucket-definitions:write</li> <li>storage:bucket-definitions:delete</li> <li>storage:bucket-definitions:truncate</li> </ul> <p>Policy definition:</p> <pre><code>ALLOW storage:bucket-definitions:write;\nALLOW storage:bucket-definitions:read;\nALLOW storage:bucket-definitions:delete;\nALLOW storage:bucket-definitions:truncate;\n</code></pre> <p></p> <p>After creating the policy, be sure to add/bind it to a group that you belong to.</p>"},{"location":"2-getting-started/#continue","title":"Continue","text":"<ul> <li>Continue to Codespaces Setup</li> </ul>"},{"location":"3-codespaces/","title":"3. Codespaces","text":""},{"location":"3-codespaces/#create-codespace","title":"Create Codespace","text":"<p>Click to open Codespaces for this lab repository:</p> <p></p> <p>Codespace Configuration</p> <ul> <li>Branch<ul> <li>select the main branch</li> </ul> </li> <li>Dev container configuration<ul> <li>select Enablement on codespaces template</li> </ul> </li> <li>Machine type<ul> <li>select 4-core</li> </ul> </li> <li>Region<ul> <li>select any region, preferably one closest to your Dynatrace tenant</li> </ul> </li> </ul>"},{"location":"3-codespaces/#wait-for-codespace","title":"Wait for Codespace","text":"<p>We know your time is very valuable. This codespace takes around 7-10 minutes to be fully operational. A local Kubernetes (kind) cluster monitored by Dynatrace will be configured and in it a sample application, Astronomy Shop, will be deployed. To make your experience better, we are also installing and configuring tools like:</p> <p>k9s kubectl helm node jq python3 gh</p>"},{"location":"3-codespaces/#explore-codespace","title":"Explore Codespace","text":"<p>Your Codespace has now deployed the following resources:</p> <ul> <li> <p>A local Kubernetes (kind) cluster monitored by Dynatrace, with some pre-deployed apps that will be used later in the demo.</p> </li> <li> <p>After a couple of minutes, you'll see this screen in your codespaces terminal. It contains the links to the locally exposed labguide and the UI of the application which we will be doing our hands-on training with.</p> </li> </ul> <p>Sample output: </p>"},{"location":"3-codespaces/#tips-tricks","title":"Tips &amp; Tricks","text":"<p>We want to boost your learning and try to make your experience as smooth as possible with Dynatrace trainings. Your Codespaces have a couple of convenience features added. </p>"},{"location":"3-codespaces/#show-the-greeting","title":"Show the greeting","text":"<p>In the terminal, there are functions loaded for your convenience. By creating a new Terminal the Greeting will be shown that includes the links to the exposed apps, the Github  pages, the Github Repository, the Dynatrace Tenant that is bound to this devcontainer and some of the tools installed.</p> <p>You can create a new Terminal directly in VSCode, type <code>zsh</code> or call the function <code>printGreeting</code> and that will print the greeting with the most relevant information.</p>"},{"location":"3-codespaces/#navigating-in-your-local-kubernetes","title":"Navigating in your local Kubernetes","text":"<p>The client <code>kubectl</code> and <code>k9s</code>are configured so you can navigate in your local Kubernetes like butter.  </p>"},{"location":"3-codespaces/#exposing-the-apps-to-the-public","title":"Exposing the apps to the public","text":"<p>The apps MKdocs and Astronomy Shop are being exposed in the devcontainer to your localhost. If you want to make the endpoints public accesible, just go to the ports section in VsCode, right click on them and change the visibility to public.</p>"},{"location":"3-codespaces/#troubleshooting","title":"Troubleshooting","text":""},{"location":"3-codespaces/#astronomy-shop","title":"Astronomy Shop","text":"<p>If you encounter problems with the Astronomy Shop app deployed in the <code>astronomy-shop</code> namespace, recycle the pods and expose the app.</p> <p>Recycle pods: <pre><code>kubectl delete pods -n astronomy-shop --field-selector=\"status.phase=Running\"\n</code></pre></p> <p>Expose app: <pre><code>exposeAstronomyShop\n</code></pre></p>"},{"location":"3-codespaces/#continue","title":"Continue","text":"<ul> <li>Continue to OpenTelemetry Logs</li> </ul>"},{"location":"4-opentelemetry-logs/","title":"OpenTelemetry Logs","text":"<p>In this module we'll utilize the OpenTelemetry Collector deployed as a DaemonSet (Node Agent) to collect pod/container logs from a Kubernetes cluster and ship them to Dynatrace.  Additionally, we'll deploy the OpenTelemetry Collector as a Deployment (Gateway) to watch Kubernetes Events from the Cluster and ship them to Dynatrace.</p> <p>Goals:</p> <ol> <li>Deploy OpenTelemetry Collector as a DaemonSet</li> <li>Deploy OpenTelemetry Collector as a Deployment</li> <li>Configure OpenTelemetry Collector service pipeline for log enrichment</li> <li>Query and visualize logs in Dynatrace using DQL</li> </ol> <p></p>"},{"location":"4-opentelemetry-logs/#prerequisites","title":"Prerequisites","text":"<p>Import Notebook into Dynatrace</p> <p>Download Notebook</p> <p>Define workshop user variables</p> <p>In your Github Codespaces Terminal: <pre><code>export DT_ENDPOINT=https://{your-environment-id}.live.dynatrace.com/api/v2/otlp\nexport DT_API_TOKEN={your-api-token}\nexport NAME=&lt;INITIALS&gt;-k8s-otel-o11y\n</code></pre></p>"},{"location":"4-opentelemetry-logs/#opentelemetry-operator","title":"OpenTelemetry Operator","text":"<p>Move to the Base Directory</p> <p>Command: <pre><code>cd $BASE_DIR\n</code></pre></p> <p>You should find yourself at the base directory of the repository. If not, then navigate to it.</p> <p>Create <code>dynatrace</code> namespace</p> <p>Command: <pre><code>kubectl create namespace dynatrace\n</code></pre></p> <p>Sample output: <pre><code>&gt; namespace/dynatrace created\n</code></pre></p> <p>Create <code>dynatrace-otelcol-dt-api-credentials</code> secret</p> <p>The secret holds the API endpoint and API token that OpenTelemetry data will be sent to.</p> <p>Command: <pre><code>kubectl create secret generic dynatrace-otelcol-dt-api-credentials --from-literal=DT_ENDPOINT=$DT_ENDPOINT --from-literal=DT_API_TOKEN=$DT_API_TOKEN -n dynatrace\n</code></pre></p> <p>Sample output:</p> <pre><code>&gt; secret/dynatrace-otelcol-dt-api-credentials created\n</code></pre> <p>Deploy <code>cert-manager</code>, pre-requisite for <code>opentelemetry-operator</code></p> <p>Cert Manager Installation Documentation</p> <p>Command: <pre><code>kubectl apply -f cluster-manifests/cert-manager.yaml\n</code></pre> Sample output:</p> <p>namespace/cert-manager created\\ customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\\ customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\\ ...\\ validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created</p> <p>View Complete Manifest</p> <p>Wait 30-60 seconds for cert-manager to finish initializing before continuing.</p> <p>Validate that the Cert Manager components are running.</p> <p>Command: <pre><code>kubectl get pods -n cert-manager\n</code></pre></p> NAME READY STATUS RESTARTS AGE cert-manager-5f7b5dbfbc-fkpzv 1/1 Running 0 1m cert-manager-cainjector-7d5b44bb96-kqz7f 1/1 Running 0 1m cert-manager-webhook-69459b8974-tsmbq 1/1 Running 0 1m <p>Deploy <code>opentelemetry-operator</code></p> <p>The OpenTelemetry Operator will deploy and manage the custom resource <code>OpenTelemetryCollector</code> deployed on the cluster.</p> <p>Command: <pre><code>kubectl apply -f cluster-manifests/opentelemetry-operator.yaml\n</code></pre></p> <p>Sample output:</p> <p>namespace/opentelemetry-operator-system created\\ customresourcedefinition.apiextensions.k8s.io/instrumentations.opentelemetry.io created\\ customresourcedefinition.apiextensions.k8s.io/opampbridges.opentelemetry.io created\\ ...\\ validatingwebhookconfiguration.admissionregistration.k8s.io/opentelemetry-operator-validating-webhook-configuration configured</p> <p>View Complete Manifest</p> <p>Wait 30-60 seconds for opentelemetry-operator-controller-manager to finish initializing before continuing.</p> <p>Validate that the OpenTelemetry Operator components are running.</p> <p>Command: <pre><code>kubectl get pods -n opentelemetry-operator-system\n</code></pre></p> NAME READY STATUS RESTARTS AGE opentelemetry-operator-controller-manager-5d746dbd64-rf9st 2/2 Running 0 1m"},{"location":"4-opentelemetry-logs/#opentelemetry-collector-for-logs","title":"OpenTelemetry Collector for Logs","text":"<p>OpenTelemetry Collector from Dynatrace Documentation</p> <p>The Collector is a network service application that you can use to batch and transform telemetry data. It acts as a proxy and can receive OTLP requests as well as data from other sources, transform these requests according to defined rules, and forward them to the backend.</p> <p></p> <p>Dynatrace offers a sample configuration for the <code>Ingest Pod Logs</code> use case:</p> <p>Ingest Pod Logs Documentation</p> <p>Move into the lab module directory</p> <p>Command: <pre><code>cd $BASE_DIR/lab-modules/opentelemetry-logs\n</code></pre></p> <p>Dynatrace Distro - Daemonset (Node Agent)</p> <p>OpenTelemetry Collector Node Agent Documentation</p> <p>Pod (and container) logs are written to the filesystem of the Node where the pod is running.  Therefore the Collector must be deployed as a Daemonset to read the log files on the local Node.</p> <pre><code>---\napiVersion: opentelemetry.io/v1beta1\nkind: OpenTelemetryCollector\nmetadata:\n  name: dynatrace-logs\n  namespace: dynatrace\nspec:\n  envFrom:\n  - secretRef:\n      name: dynatrace-otelcol-dt-api-credentials\n  mode: \"daemonset\"\n  image: \"ghcr.io/dynatrace/dynatrace-otel-collector/dynatrace-otel-collector:latest\"\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/logs/otel-collector-logs-crd-01.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-logs created</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-logs-collector-8q8tz 1/1 Running 0 1m"},{"location":"4-opentelemetry-logs/#filelog-receiver","title":"<code>filelog</code> Receiver","text":"<p>Filelog Receiver Documentation</p> <p>The Filelog Receiver tails and parses logs from files. Although it\u2019s not a Kubernetes-specific receiver, it is still the de facto solution for collecting any logs from Kubernetes.  Logs from the Kubernetes Node's filesystem will be read from the Collector running on that Node.  This is why the Collector is deployed as a Daemonset and not a Deployment (or Sidecar).</p> <p>The Filelog Receiver is composed of Operators that are chained together to process a log. Each Operator performs a simple responsibility, such as parsing a timestamp or JSON. Configuring a Filelog Receiver is not trivial.  Refer to the documentation for details.</p> <pre><code>config:\n    receivers:\n      filelog:\n        ...\n    service:\n      pipelines:\n        logs:\n          receivers: [filelog]\n          processors: [batch]\n          exporters: [otlphttp/dynatrace]\n</code></pre> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter isNotNull(log.file.path)\n| sort timestamp desc\n| limit 100\n| fields timestamp, loglevel, status, k8s.namespace.name, k8s.pod.name, k8s.container.name, content, log.file.path\n</code></pre> Result:</p> <p></p>"},{"location":"4-opentelemetry-logs/#k8sattributes-processor","title":"<code>k8sattributes</code> Processor","text":"<p>Add Kubernetes Attributes with the <code>k8sattributes</code> Processor</p> <p>k8sattributes Processor Documentation</p> <p>The Kubernetes Attributes Processor automatically discovers Kubernetes pods, extracts their metadata, and adds the extracted metadata to spans, metrics, and logs as resource attributes.</p> <p>The Kubernetes Attributes Processor is one of the most important components for a collector running in Kubernetes. Any collector receiving application data should use it. Because it adds Kubernetes context to your telemetry, the Kubernetes Attributes Processor lets you correlate your application\u2019s traces, metrics, and logs signals with your Kubernetes telemetry, such as pod metrics and traces.</p> <p>Create <code>clusterrole</code> with read access to Kubernetes objects</p> <p>Since the processor uses the Kubernetes API, it needs the correct permission to work correctly. For most use cases, you should give the service account running the collector the following permissions via a ClusterRole.</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otel-collector-k8s-clusterrole-logs\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"namespaces\", \"nodes\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"replicasets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"replicasets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/rbac/otel-collector-k8s-clusterrole-logs.yaml\n</code></pre></p> <p>Sample output:</p> <p>clusterrole.rbac.authorization.k8s.io/otel-collector-k8s-clusterrole-logs created</p> <p>View Complete Manifest</p> <p>Create <code>clusterrolebinding</code> for OpenTelemetry Collector service account</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otel-collector-k8s-clusterrole-logs-crb\nsubjects:\n- kind: ServiceAccount\n  name: dynatrace-logs-collector\n  namespace: dynatrace\nroleRef:\n  kind: ClusterRole\n  name: otel-collector-k8s-clusterrole-logs\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/rbac/otel-collector-k8s-clusterrole-logs-crb.yaml\n</code></pre></p> <p>Sample output:</p> <p>clusterrolebinding.rbac.authorization.k8s.io/otel-collector-k8s-clusterrole-logs-crb created</p> <p>View Complete Manifest</p> <p>Add <code>k8sattributes</code> processor</p> <pre><code>k8sattributes:\n    auth_type: \"serviceAccount\"\n    passthrough: false\n        filter:\n        node_from_env_var: KUBE_NODE_NAME\n    extract:\n        metadata:\n            - k8s.namespace.name\n            - k8s.deployment.name\n            - k8s.daemonset.name\n            - k8s.job.name\n            - k8s.cronjob.name\n            - k8s.replicaset.name\n            - k8s.statefulset.name\n            - k8s.pod.name\n            - k8s.pod.uid\n            - k8s.node.name\n            - k8s.container.name\n            - container.id\n            - container.image.name\n            - container.image.tag\n        labels:\n        - tag_name: app.label.component\n            key: app.kubernetes.io/component\n            from: pod\n    pod_association:\n        - sources:\n            - from: resource_attribute\n              name: k8s.pod.uid\n        - sources:\n            - from: resource_attribute\n              name: k8s.pod.name\n        - sources:\n            - from: resource_attribute\n              name: k8s.pod.ip\n        - sources:\n            - from: connection\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/logs/otel-collector-logs-crd-02.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-logs configured</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-logs-collector-dns4x 1/1 Running 0 1m <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"astronomy-shop\" and isNotNull(k8s.deployment.name)\n| sort timestamp desc\n| limit 100\n| fields timestamp, loglevel, status, k8s.namespace.name, k8s.deployment.name, k8s.pod.name, k8s.container.name, app.label.component, content\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"4-opentelemetry-logs/#resourcedetection-processor","title":"<code>resourcedetection</code> Processor","text":"<p>The resource detection processor can be used to detect resource information from the host, in a format that conforms to the OpenTelemetry resource semantic conventions, and append or override the resource value in telemetry data with this information.  Detectors are available for AWS, Azure, GCP, and several other platforms; see the documentation for more details.</p> <p>This processor is a great plugin for adding attributes such as <code>cloud.account.id</code> and <code>k8s.cluster.name</code> to the telemetry.</p> <p>Add <code>resourcedetection</code> processor</p> <p>Resource Detection Processor Documentation</p> <pre><code>processors:\n  resourcedetection/gcp:\n    detectors: [env, gcp]\n    timeout: 2s\n    override: false\n</code></pre> <p>note: for this lab, the Kind cluster does not have cloud metadata to collect.  These values will be spoofed for the purposes of this lab.</p> <pre><code>resource/kind:\n  attributes:\n  - key: cloud.account.id\n    value: dt-k8s-o11y-account\n    action: insert\n  - key: k8s.cluster.name\n    value: dt-k8s-o11y-kind\n    action: insert\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/logs/otel-collector-logs-crd-03.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-logs configured</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-logs-collector-fbtk5 1/1 Running 0 1m <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter isNotNull(cloud.account.id) and isNotNull(k8s.cluster.name)\n| filter k8s.namespace.name == \"astronomy-shop\" and isNotNull(k8s.deployment.name)\n| sort timestamp desc\n| limit 100\n| fields timestamp, loglevel, status, cloud.account.id, k8s.cluster.name, k8s.namespace.name, k8s.deployment.name, content\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"4-opentelemetry-logs/#resource-processor","title":"<code>resource</code> Processor","text":"<p>Add <code>resource</code> processor (attributes)</p> <p>Resource Processor Documentation</p> <p>The <code>resource</code> processor allows us to directly add, remove, or change resource attributes on the telemetry.  View the documentation for more details.</p> <p>We will use this processor to make the follow changes to our telemetry: * <code>k8s.pod.ip</code> values in our log data are either the same or invalid; delete the useless attribute * <code>telemetry.sdk.name</code> set to <code>opentelemetry</code> will allow us to easily identify logs captured through OpenTelemetry * <code>dynatrace.otel.collector</code> is a non-standardized attribute that we made up to help us identify which Collector captured this data * <code>dt.security_context</code> is a Dynatrace specific attribute that we use to manage user permissions to the telemetry     * This could also be set using OpenPipeline, but this puts control of this attribute's value at the app/infra layer (optionally)</p> <p><pre><code>processors:\n    resource:\n        attributes:\n        - key: k8s.pod.ip\n          action: delete\n        - key: telemetry.sdk.name\n          value: opentelemetry\n          action: insert\n        - key: dynatrace.otel.collector\n          value: dynatrace-logs\n          action: insert\n        - key: dt.security_context\n          from_attribute: k8s.cluster.name\n          action: insert\n</code></pre> Command: <pre><code>kubectl apply -f opentelemetry/collector/logs/otel-collector-logs-crd-04.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-logs configured</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-logs-collector-xx6km 1/1 Running 0 1m <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter isNotNull(dt.security_context)\n| filter isNotNull(cloud.account.id) and isNotNull(k8s.cluster.name)\n| filter k8s.namespace.name == \"astronomy-shop\" and isNotNull(k8s.deployment.name)\n| sort timestamp desc\n| limit 100\n| fields timestamp, loglevel, status, dt.security_context, dynatrace.otel.collector, cloud.account.id, k8s.cluster.name, k8s.namespace.name, k8s.deployment.name, content\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"4-opentelemetry-logs/#export-to-otlp-receiver","title":"Export to OTLP Receiver","text":"<p>The <code>astronomy-shop</code> demo application has the OpenTelemetry agents and SDKs already instrumented.  These agents and SDKs are generating logs (traces and metrics too) that are being exported to a Collector running within the <code>astronomy-shop</code> namespace bundled into the application deployment.  We want these logs to be shipped to Dynatrace as well.</p> <p>OTLP Receiver Documentation</p> <p>Adding the <code>otlp</code> receiver allows us to receive telemetry from otel exporters, such as agents and other collectors.</p> <pre><code>config:\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n            endpoint: 0.0.0.0:4317\n          http:\n            endpoint: 0.0.0.0:4318\n    service:\n      pipelines:\n        logs:\n          receivers: [otlp]\n          processors: [batch]\n          exporters: [otlphttp/dynatrace]\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/logs/otel-collector-logs-crd-05.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-logs configured</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-logs-collector-gu0rm 1/1 Running 0 1m <p>Customize astronomy-shop helm values</p> <p>OpenTelemetry data created by agents and SDKs should include <code>service.name</code> and <code>service.namespace</code> attributes.  We will make the <code>service.namespace</code> unique to our deployment using our <code>NAME</code> environment variable declared earlier, using a <code>sed</code> command on the Helm chart's <code>values.yaml</code> file.</p> <p><pre><code>default:\n  # List of environment variables applied to all components\n  env:\n    - name: OTEL_SERVICE_NAME\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: \"metadata.labels['app.kubernetes.io/component']\"\n    - name: OTEL_COLLECTOR_NAME\n      value: '{{ include \"otel-demo.name\" . }}-otelcol'\n    - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE\n      value: cumulative\n    - name: OTEL_RESOURCE_ATTRIBUTES\n      value: 'service.name=$(OTEL_SERVICE_NAME),service.namespace=NAME_TO_REPLACE,service.version={{ .Chart.AppVersion }}'\n</code></pre> <code>service.namespace=NAME_TO_REPLACE</code></p> <p>service.namespace=INITIALS-k8s-otel-o11y</p> <p>Command: <pre><code>sed -i \"s,NAME_TO_REPLACE,$NAME,\" astronomy-shop/collector-values.yaml\n</code></pre></p>"},{"location":"4-opentelemetry-logs/#update-astronomy-shop-via-helm","title":"Update Astronomy Shop via Helm","text":"<p>Our <code>collector-values.yaml</code> contains new configurations for the application so that the <code>astronomy-shop</code> Collector includes exporters that ship to the Collectors deployed in the <code>dynatrace</code> namespace.</p> <pre><code>exporters:\n  # Dynatrace OTel Collectors\n  otlphttp/dttraces:\n    endpoint: http://dynatrace-traces-collector.dynatrace.svc.cluster.local:4318\n  otlphttp/dtlogs:\n    endpoint: http://dynatrace-logs-collector.dynatrace.svc.cluster.local:4318\n  otlphttp/dtmetrics:\n    endpoint: http://dynatrace-metrics-cluster-collector.dynatrace.svc.cluster.local:4318\n</code></pre> <p>Command: <pre><code>helm upgrade astronomy-shop open-telemetry/opentelemetry-demo --values astronomy-shop/collector-values.yaml --namespace astronomy-shop --version \"0.31.0\"\n</code></pre></p> <p>Sample output:</p> <p>NAME: astronomy-shop\\ LAST DEPLOYED: Thu Jun 27 20:58:38 2024\\ NAMESPACE: astronomy-shop\\ STATUS: deployed\\ REVISION: 2</p> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"astronomy-shop\" and isNotNull(service.name)\n| sort timestamp desc\n| limit 100\n| fields timestamp, content, k8s.cluster.name, k8s.pod.name, service.namespace, service.name, telemetry.sdk.language, otel.scope.name\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"4-opentelemetry-logs/#opentelemetry-collector-for-events","title":"OpenTelemetry Collector for Events","text":"<p>The Kubernetes Objects receiver collects, either by pulling or watching, objects from the Kubernetes API server. The most common use case for this receiver is watching Kubernetes events, but it can be used to collect any type of Kubernetes object.</p>"},{"location":"4-opentelemetry-logs/#k8sobjects-receiver","title":"<code>k8sobjects</code> Receiver","text":"<p>Kubernetes Objects Receiver Documentation</p> <p>Our goal is to capture any events related to the <code>astronomy-shop</code> and <code>dynatrace</code> namespaces.</p> <pre><code>receivers:\n  k8sobjects/events:\n    auth_type: serviceAccount\n    objects:\n      - name: events\n        mode: watch\n        namespaces: [astronomy-shop,dynatrace]\n</code></pre> <p>The <code>k8sobjects</code> receiver is only available on the Contrib Distro of the OpenTelemetry Collector.  Therefore we must deploy a new Collector using the <code>contrib</code> image.</p> <p>Create <code>clusterrole</code> with read access to Kubernetes events</p> <p>Since the processor uses the Kubernetes API, it needs the correct permission to work correctly. Since service accounts are the only authentication option you must give the service account the proper access. For any object you want to collect you need to ensure the name is added to the cluster role. </p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otel-collector-k8s-clusterrole-events\nrules:\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/rbac/otel-collector-k8s-clusterrole-events.yaml\n</code></pre></p> <p>Sample output:</p> <p>clusterrole.rbac.authorization.k8s.io/otel-collector-k8s-clusterrole-events created</p> <p>View Complete Manifest</p> <p>Create <code>clusterrolebinding</code> for OpenTelemetry Collector service account</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otel-collector-k8s-clusterrole-events-crb\nsubjects:\n- kind: ServiceAccount\n  name: dynatrace-events-collector\n  namespace: dynatrace\nroleRef:\n  kind: ClusterRole\n  name: otel-collector-k8s-clusterrole-events\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/rbac/otel-collector-k8s-clusterrole-events-crb.yaml\n</code></pre></p> <p>Sample output:</p> <p>clusterrolebinding.rbac.authorization.k8s.io/otel-collector-k8s-clusterrole-events-crb created</p> <p>View Complete Manifest</p> <p>Add <code>k8sobjects</code> receiver to collect Kubernetes events as logs</p> <pre><code>receivers:\n  k8sobjects/events:\n    auth_type: serviceAccount\n    objects:\n      - name: events\n        mode: watch\n        namespaces: [astronomy-shop,dynatrace]\n</code></pre>"},{"location":"4-opentelemetry-logs/#deploy-opentelemetry-collector-gateway","title":"Deploy OpenTelemetry Collector (Gateway)","text":"<p>Since the receiver gathers telemetry for the cluster as a whole, only one instance of the receiver is needed across the cluster in order to collect all the data.</p> <pre><code>---\napiVersion: opentelemetry.io/v1beta1\nkind: OpenTelemetryCollector\nmetadata:\n  name: dynatrace-events\n  namespace: dynatrace\nspec:\n  envFrom:\n  - secretRef:\n      name: dynatrace-otelcol-dt-api-credentials\n  mode: \"deployment\"\n  image: \"otel/opentelemetry-collector-contrib:latest\"\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/events/otel-collector-events-crd-01.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-events created</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-events-collector-559d5b9d77-rb26d 1/1 Running 0 1m"},{"location":"4-opentelemetry-logs/#generate-events","title":"Generate Events","text":"<p>Kubectl Scale Command Documentation</p> <p>We can generate new Kubernetes events related to the <code>astronomy-shop</code> namespace by scaling a deployment up and then scaling it back down.</p> <p>Command: <pre><code>kubectl scale deployment astronomy-shop-imageprovider -n astronomy-shop --replicas=2\n</code></pre></p> <p>Sample output:</p> <p>deployment.apps/astronomy-shop-imageprovider scaled</p> <p>Command: <pre><code>kubectl get pods -n astronomy-shop -l app.kubernetes.io/name=astronomy-shop-imageprovider\n</code></pre></p> NAME READY STATUS RESTARTS AGE astronomy-shop-imageprovider-84b549cf4c-nwm7z 2/2 Running 0 9m astronomy-shop-imageprovider-84b549cf4c-rs7rz 2/2 Running 0 1m <p>Command: <pre><code>kubectl scale deployment astronomy-shop-imageprovider -n astronomy-shop --replicas=1\n</code></pre></p> <p>Sample output:</p> <p>deployment.apps/astronomy-shop-imageprovider scaled</p> <p>Command: <pre><code>kubectl get pods -n astronomy-shop -l app.kubernetes.io/name=astronomy-shop-imageprovider\n</code></pre></p> NAME READY STATUS RESTARTS AGE astronomy-shop-imageprovider-84b549cf4c-nwm7z 2/2 Running 0 9m <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter dynatrace.otel.collector == \"dynatrace-events\" and event.domain == \"k8s\" and k8s.resource.name == \"events\"\n| filter object.metadata.namespace == \"astronomy-shop\"\n| sort timestamp desc\n| limit 100\n| fields timestamp, k8s.cluster.name, {object.metadata.namespace, alias: k8s.namespace.name}, object.message, object.reason, event.name\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"4-opentelemetry-logs/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully deployed the OpenTelemetry Collector to collect logs, enrich log attributes for better context, and ship those logs to Dynatrace for analysis.</p> <ul> <li>The OpenTelemetry Collector was deployed as a DaemonSet, behaving as an Agent running on each Node</li> <li>The Dynatrace Distro of OpenTelemetry Collector includes supported modules needed to ship logs to Dynatrace<ul> <li>The <code>filelog</code> receiver scrapes logs from the Node filesystem and parses the contents</li> <li>The <code>otlp</code> receiver receives logs that are exported from agents, SDKs, and other Collectors</li> <li>The <code>k8sattributes</code> processor enriches the logs with Kubernetes attributes</li> <li>The <code>resourcedetection</code> processor enriches the logs with cloud and cluster attributes</li> <li>The <code>resource</code> processor enriches the logs with custom (resource) attributes</li> </ul> </li> <li>The Community Contrib Distro of OpenTelemetry Collector includes modules needed to ship events to Dynatrace<ul> <li>The <code>k8sobjects</code> receiver watches for Kubernetes events (and other resources) on the cluster</li> </ul> </li> <li>Dynatrace DQL (via Notebooks) allows you to perform powerful queries and analysis of the log data</li> </ul>"},{"location":"4-opentelemetry-logs/#continue","title":"Continue","text":"<p>Next we'll deploy the OpenTelemetry Capstone to capture logs, metrics, and traces with OpenTelemetry Collectors.</p> <ul> <li>Continue to OpenTelemetry Capstone</li> </ul>"},{"location":"5-opentelemetry-capstone/","title":"OpenTelemetry Capstone","text":"<p>In this module we'll utilize multiple OpenTelemetry Collectors to collect application traces/spans, log records, and metric data points generated by OpenTelemetry, from a Kubernetes cluster and ship them to Dynatrace.  This is a capstone lab that utilizes the concepts of the previous Kubernetes OpenTelemetry labs.</p> <p>Goals:</p> <ol> <li>Deploy 4 OpenTelemetry Collectors</li> <li>Configure OpenTelemetry Collector service pipeline for data enrichment</li> <li>Analyze metrics, traces, and logs in Dynatrace</li> </ol>"},{"location":"5-opentelemetry-capstone/#prerequisites","title":"Prerequisites","text":"<p>Import Dashboards into Dynatrace</p> <p> Download astronomy-shop Dashboard</p> <p> Download Collector Health Dashboard</p> <p>Define workshop user variables</p> <p>In your Github Codespaces Terminal: <pre><code>export DT_ENDPOINT=https://{your-environment-id}.live.dynatrace.com/api/v2/otlp\nexport DT_API_TOKEN={your-api-token}\nexport NAME=&lt;INITIALS&gt;-k8s-otel-o11y\n</code></pre></p> <p>Move into the lab module directory</p> <p>Command:</p> <pre><code>cd $BASE_DIR/lab-modules/opentelemetry-capstone\n</code></pre> <p>Automate Capstone Deployment</p> <p>If you've already completed the capstone deployment before, you can automate the deployment using the function provided <code>deployOpenTelemetryCapstone</code> in your terminal shell.</p> <p>Clean Up Previous Deployments</p> <p>Delete <code>dynatrace</code> namespace and all previous Collector deployments</p> <p>Command: <pre><code>kubectl delete ns dynatrace\n</code></pre></p> <p>Create <code>dynatrace</code> namespace</p> <p>Command: <pre><code>kubectl create namespace dynatrace\n</code></pre></p> <p>Sample output:</p> <p>namespace/dynatrace created</p> <p>Create <code>dynatrace-otelcol-dt-api-credentials</code> secret</p> <p>The secret holds the API endpoint and API token that OpenTelemetry data will be sent to.</p> <p>Command: <pre><code>kubectl create secret generic dynatrace-otelcol-dt-api-credentials --from-literal=DT_ENDPOINT=$DT_ENDPOINT --from-literal=DT_API_TOKEN=$DT_API_TOKEN -n dynatrace\n</code></pre></p> <p>Sample output:</p> <p>secret/dynatrace-otelcol-dt-api-credentials created</p> <p>Deploy <code>cert-manager</code>, pre-requisite for <code>opentelemetry-operator</code></p> <p>Cert Manager Documentation</p> <p>Command: <pre><code>kubectl apply -f opentelemetry/cert-manager.yaml\n</code></pre></p> <p>Sample output:</p> <p>namespace/cert-manager created\\ customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\\ customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\\ ...\\ validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created</p> <p>View Complete Manifest</p> <p>Wait 30-60 seconds for cert-manager to finish initializing before continuing.</p> <p>Validate that the Cert Manager components are running.</p> <p>Command: <pre><code>kubectl get pods -n cert-manager\n</code></pre></p> <p>Sample output:</p> NAME READY STATUS RESTARTS AGE cert-manager-5f7b5dbfbc-fkpzv 1/1 Running 0 1m cert-manager-cainjector-7d5b44bb96-kqz7f 1/1 Running 0 1m cert-manager-webhook-69459b8974-tsmbq 1/1 Running 0 1m"},{"location":"5-opentelemetry-capstone/#opentelemetry-operator-and-role-based-access","title":"OpenTelemetry Operator and Role Based Access","text":"<p>Deploy <code>opentelemetry-operator</code></p> <p>The OpenTelemetry Operator will deploy and manage the custom resource <code>OpenTelemetryCollector</code> deployed on the cluster.</p> <p>Command: <pre><code>kubectl apply -f opentelemetry/opentelemetry-operator.yaml\n</code></pre></p> <p>Sample output:</p> <p>namespace/opentelemetry-operator-system created\\ customresourcedefinition.apiextensions.k8s.io/instrumentations.opentelemetry.io created\\ customresourcedefinition.apiextensions.k8s.io/opampbridges.opentelemetry.io created\\ ...\\ validatingwebhookconfiguration.admissionregistration.k8s.io/opentelemetry-operator-validating-webhook-configuration configured</p> <p>View Complete Manifest</p> <p>Wait 30-60 seconds for opentelemetry-operator-controller-manager to finish initializing before continuing.</p> <p>Validate that the OpenTelemetry Operator components are running.</p> <p>Command: <pre><code>kubectl get pods -n opentelemetry-operator-system\n</code></pre></p> <p>Sample output:</p> NAME READY STATUS RESTARTS AGE opentelemetry-operator-controller-manager-5d746dbd64-rf9st 2/2 Running 0 1m <p>Create <code>clusterrole</code> with read access to Kubernetes objects</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otel-collector-k8s-clusterrole\nrules:\n  - apiGroups: ['']\n    resources: ['events', 'namespaces', 'namespaces/status', 'nodes', 'nodes/spec', 'nodes/stats', 'nodes/proxy', 'pods', 'pods/status', 'replicationcontrollers', 'replicationcontrollers/status', 'resourcequotas', 'services']\n    verbs: ['get', 'list', 'watch']\n  - apiGroups: ['apps']\n    resources: ['daemonsets', 'deployments', 'replicasets', 'statefulsets']\n    verbs: ['get', 'list', 'watch']\n  - apiGroups: ['extensions']\n    resources: ['daemonsets', 'deployments', 'replicasets']\n    verbs: ['get', 'list', 'watch']\n  - apiGroups: ['batch']\n    resources: ['jobs', 'cronjobs']\n    verbs: ['get', 'list', 'watch']\n  - apiGroups: ['autoscaling']\n    resources: ['horizontalpodautoscalers']\n    verbs: ['get', 'list', 'watch']\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/rbac/otel-collector-k8s-clusterrole.yaml\n</code></pre></p> <p>Sample output:</p> <p>clusterrole.rbac.authorization.k8s.io/otel-collector-k8s-clusterrole created</p> <p>View Complete Manifest</p> <p>Create <code>clusterrolebinding</code> for OpenTelemetry Collector service accounts</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otel-collector-k8s-clusterrole-crb\nsubjects:\n- kind: ServiceAccount\n  name: dynatrace-deployment-collector\n  namespace: dynatrace\n- kind: ServiceAccount\n  name: dynatrace-daemonset-collector\n  namespace: dynatrace\n- kind: ServiceAccount\n  name: contrib-deployment-collector\n  namespace: dynatrace\n- kind: ServiceAccount\n  name: contrib-daemonset-collector\n  namespace: dynatrace\nroleRef:\n  kind: ClusterRole\n  name: otel-collector-k8s-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/rbac/otel-collector-k8s-clusterrole-crb.yaml\n</code></pre></p> <p>Sample output:</p> <p>clusterrolebinding.rbac.authorization.k8s.io/otel-collector-k8s-clusterrole-crb created</p> <p>View Complete Manifest</p>"},{"location":"5-opentelemetry-capstone/#dynatrace-deployment-collector","title":"Dynatrace Deployment Collector","text":"<p>Receivers: <code>otlp</code>, <code>prometheus</code></p> MODULE DT DEPLOY DT DAEMON CON DEPLOY CON DAEMON otlp - [x] - [ ] - [x] - [ ] prometheus - [x] - [x] - [x] - [x] filelog - [ ] - [x] - [ ] - [ ] kubeletstats - [ ] - [ ] - [ ] - [x] k8s_cluster - [ ] - [ ] - [x] - [ ] k8sobjects - [ ] - [ ] - [x] - [ ] <p>Deploy OpenTelemetry Collector CRD</p> <p>Deployment Documentation</p> <pre><code>---\napiVersion: opentelemetry.io/v1beta1\nkind: OpenTelemetryCollector\nmetadata:\n  name: dynatrace-deployment\n  namespace: dynatrace\nspec:\n  envFrom:\n  - secretRef:\n      name: dynatrace-otelcol-dt-api-credentials\n  mode: \"deployment\"\n  image: \"ghcr.io/dynatrace/dynatrace-otel-collector/dynatrace-otel-collector:latest\"\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/dynatrace/otel-collector-dynatrace-deployment-crd.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-deployment created</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> <p>Sample output:</p> NAME READY STATUS RESTARTS AGE dynatrace-deployment-collector-796546fbd6-kqflf 1/1 Running 0 1m"},{"location":"5-opentelemetry-capstone/#dynatrace-daemonset-collector","title":"Dynatrace Daemonset Collector","text":"<p>Receivers: <code>filelog</code>, <code>prometheus</code></p> MODULE DT DEPLOY DT DAEMON CON DEPLOY CON DAEMON otlp - [x] - [ ] - [x] - [ ] prometheus - [x] - [x] - [x] - [x] filelog - [ ] - [x] - [ ] - [ ] kubeletstats - [ ] - [ ] - [ ] - [x] k8s_cluster - [ ] - [ ] - [x] - [ ] k8sobjects - [ ] - [ ] - [x] - [ ] <p>Deploy OpenTelemetry Collector CRD</p> <p>Daemonset Documentation</p> <pre><code>---\napiVersion: opentelemetry.io/v1beta1\nkind: OpenTelemetryCollector\nmetadata:\n  name: dynatrace-daemonset\n  namespace: dynatrace\nspec:\n  envFrom:\n  - secretRef:\n      name: dynatrace-otelcol-dt-api-credentials\n  mode: \"daemonset\"\n  image: \"ghcr.io/dynatrace/dynatrace-otel-collector/dynatrace-otel-collector:latest\"\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/dynatrace/otel-collector-dynatrace-daemonset-crd.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/dynatrace-daemonset created</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> <p>Sample output:</p> NAME READY STATUS RESTARTS AGE dynatrace-daemonset-collector-h69pz 1/1 Running 0 1m"},{"location":"5-opentelemetry-capstone/#contrib-deployment-collector","title":"Contrib Deployment Collector","text":"<p>Receivers: <code>otlp</code>, <code>prometheus</code>, <code>k8s_cluster</code>, <code>k8sobjects</code></p> MODULE DT DEPLOY DT DAEMON CON DEPLOY CON DAEMON otlp - [x] - [ ] - [x] - [ ] prometheus - [x] - [x] - [x] - [x] filelog - [ ] - [x] - [ ] - [ ] kubeletstats - [ ] - [ ] - [ ] - [x] k8s_cluster - [ ] - [ ] - [x] - [ ] k8sobjects - [ ] - [ ] - [x] - [ ] <p>Deploy OpenTelemetry Collector CRD</p> <pre><code>---\napiVersion: opentelemetry.io/v1beta1\nkind: OpenTelemetryCollector\nmetadata:\n  name: contrib-deployment\n  namespace: dynatrace\nspec:\n  envFrom:\n  - secretRef:\n      name: dynatrace-otelcol-dt-api-credentials\n  mode: \"deployment\"\n  image: \"otel/opentelemetry-collector-contrib:0.103.0\"\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/contrib/otel-collector-contrib-deployment-crd.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/contrib-deployment created</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> <p>Sample output:</p> NAME READY STATUS RESTARTS AGE contrib-deployment-collector-74dfc4d9f4-s97k6 1/1 Running 0 1m"},{"location":"5-opentelemetry-capstone/#contrib-daemonset-collector","title":"Contrib Daemonset Collector","text":"<p>Receivers: <code>prometheus</code>, <code>kubeletstats</code></p> MODULE DT DEPLOY DT DAEMON CON DEPLOY CON DAEMON otlp - [x] - [ ] - [x] - [ ] prometheus - [x] - [x] - [x] - [x] filelog - [ ] - [x] - [ ] - [ ] kubeletstats - [ ] - [ ] - [ ] - [x] k8s_cluster - [ ] - [ ] - [x] - [ ] k8sobjects - [ ] - [ ] - [x] - [ ] <p>Deploy OpenTelemetry Collector CRD</p> <pre><code>---\napiVersion: opentelemetry.io/v1beta1\nkind: OpenTelemetryCollector\nmetadata:\n  name: contrib-daemonset\n  namespace: dynatrace\nspec:\n  envFrom:\n  - secretRef:\n      name: dynatrace-otelcol-dt-api-credentials\n  mode: \"daemonset\"\n  image: \"otel/opentelemetry-collector-contrib:0.103.0\"\n</code></pre> <p>Command: <pre><code>kubectl apply -f opentelemetry/collector/contrib/otel-collector-contrib-daemonset-crd.yaml\n</code></pre></p> <p>Sample output:</p> <p>opentelemetrycollector.opentelemetry.io/contrib-daemonset created</p> <p>View Complete Manifest</p> <p>Validate running pod(s)</p> <p>Command: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> <p>Sample output:</p> NAME READY STATUS RESTARTS AGE contrib-daemonset-collector-d92tw 1/1 Running 0 1m"},{"location":"5-opentelemetry-capstone/#configure-astronomy-shop-otlp-export","title":"Configure Astronomy Shop OTLP Export","text":"<p>The <code>astronomy-shop</code> application includes an embedded OpenTelemetry Collector.  It needs to be configured to export signals via OTLP to the Dynatrace Deployment Collector.</p> <p>Our Helm chart values manifest contains the following configuration:</p> <pre><code>config:\n    receivers:\n      otlp:\n        protocols:\n          http:\n            # Since this collector needs to receive data from the web, enable cors for all origins\n            # `allowed_origins` can be refined for your deployment domain\n            cors:\n              allowed_origins:\n                - \"http://*\"\n                - \"https://*\"\n      httpcheck/frontendproxy:\n        targets:\n          - endpoint: 'http://{{ include \"otel-demo.name\" . }}-frontendproxy:8080'\n\n    exporters:\n      # Dynatrace OTel Collector\n      otlphttp/dynatrace:\n        endpoint: http://dynatrace-deployment-collector.dynatrace.svc.cluster.local:4318\n\n    processors:\n      resource:\n        attributes:\n        - key: service.instance.id\n          from_attribute: k8s.pod.uid\n          action: insert\n\n    connectors:\n      spanmetrics: {}\n\n    service:\n      pipelines:\n        traces:\n          receivers: [otlp]\n          processors: [memory_limiter, resource, batch]\n          exporters: [spanmetrics, otlphttp/dynatrace]\n        metrics:\n          receivers: [httpcheck/frontendproxy, otlp, spanmetrics]\n          processors: [memory_limiter, resource, batch]\n          exporters: [otlphttp/dynatrace]\n        logs:\n          processors: [memory_limiter, resource, batch]\n          exporters: [otlphttp/dynatrace]\n</code></pre> <p>Customize <code>astronomy-shop</code> helm values:</p> <pre><code>default:\n  # List of environment variables applied to all components\n  env:\n    - name: OTEL_SERVICE_NAME\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: \"metadata.labels['app.kubernetes.io/component']\"\n    - name: OTEL_COLLECTOR_NAME\n      value: '{{ include \"otel-demo.name\" . }}-otelcol'\n    - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE\n      value: cumulative\n    - name: OTEL_RESOURCE_ATTRIBUTES\n      value: 'service.name=$(OTEL_SERVICE_NAME),service.namespace=NAME_TO_REPLACE,service.version={{ .Chart.AppVersion }}'\n</code></pre> <p>Pattern to replace: <pre><code>service.namespace=NAME_TO_REPLACE\n</code></pre></p> <p>Command: <pre><code>sed -i \"s,NAME_TO_REPLACE,$NAME,\" astronomy-shop/collector-values.yaml\n</code></pre></p> <p>Update <code>astronomy-shop</code> OpenTelemetry Collector export endpoint via helm</p> <p>Command: <pre><code>helm upgrade astronomy-shop open-telemetry/opentelemetry-demo --values astronomy-shop/collector-values.yaml --namespace astronomy-shop --version \"0.31.0\"\n</code></pre></p> <p>Sample output:</p> <p>NAME: astronomy-shop LAST DEPLOYED: Thu Jun 27 20:58:38 2024 NAMESPACE: astronomy-shop STATUS: deployed REVISION: 2</p>"},{"location":"5-opentelemetry-capstone/#validate-and-analyze-data-in-dynatrace","title":"Validate and Analyze Data in Dynatrace","text":"<p>Validate the OpenTelemetry data using the Astronomy Shop Dashboard.</p> <p></p> <p>Download astronomy-shop Dashboard</p>"},{"location":"5-opentelemetry-capstone/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully deployed the OpenTelemetry Collector to collect metrics, traces, and logs from Kubernetes and ship them to Dynatrace for analysis.</p> <ul> <li>The Dynatrace Distro of OpenTelemetry Collector includes supported modules needed to ship telemetry to Dynatrace<ul> <li>The <code>otlp</code> receiver receives metrics, traces, and logs from OpenTelemetry exporters via gRPC/HTTP</li> <li>The <code>filelog</code> receiver scrapes logs from the Node filesystem and parses the contents</li> <li>The <code>prometheus</code> receiver scrapes metric data exposed by Pod Prometheus endpoints</li> </ul> </li> <li>The Contrib Distro of OpenTelemetry Collector includes additional modules needed to ship telemetry to Dynatrace<ul> <li>The <code>kubeletstats</code> receiver scrapes metrics from the local kubelet on the Node</li> <li>The <code>k8s_cluster</code> receiver queries the Kubernetes cluster API to retrieve metrics</li> <li>The <code>k8sobjects</code> receiver watches for Kubernetes events (and other resources) on the cluster</li> </ul> </li> <li>Dynatrace allows you to perform powerful queries and analysis of the telemetry data</li> </ul>"},{"location":"5-opentelemetry-capstone/#continue","title":"Continue","text":"<ul> <li>Continue to Configure Dynatrace Buckets (Storage Management)</li> </ul>"},{"location":"6-dynatrace-buckets/","title":"Dynatrace Buckets (Storage Management)","text":"<p>In this module we'll create Grail storage Buckets to manage log data.  By default, all logs are stored in a default logs bucket in Grail - that retains log data for 35 days.  We can create custom buckets with different retention periods to solve for our log management and analytics use cases.</p> <p>You can create a bucket tailored to your needs. Grail buckets behave like folders in a file system and are designed for records that should be handled together. For example, you might need to store together:</p> <ul> <li>Data with the same retention period</li> <li>Data that needs to be queried/analyzed together</li> <li>Data that needs to be deleted at the same time</li> </ul> <p>Defining buckets can improve query performance by reducing query execution time and the scope of data read. Finally, having your data stored in a bucket streamlines your permission management because you can easily provide a user group or single users with access to needed data.</p> <ul> <li>Learn More</li> </ul> <p>Goals:</p> <ol> <li>View existing buckets within Grail storage management</li> <li>Create new custom buckets for log data</li> </ol>"},{"location":"6-dynatrace-buckets/#view-buckets","title":"View Buckets","text":"<p>In your Dynatrace tenant, use the Search function (<code>cmd + k</code> / <code>ctrl + k</code>) and search for <code>bucket</code>.  Click to open <code>Bucket storage management</code> under the Settings category.</p> <p></p> <p>This will open the <code>Settings</code> app and the Buckets list under the Storage management menu.  Here you can view all existing buckets, including the default buckets and custom buckets.  Each bucket has a retention period (in days) and belongs to a specific table, where the table is a specific data type.</p> <p></p> <p>Change the <code>Select bucket type</code> dropdown to filter on <code>logs</code> buckets only.  Locate the <code>default_logs</code> bucket and notice the retention period of 35 days.  By default, all logs are stored in this bucket and will be retained for 35 days.</p> <p></p>"},{"location":"6-dynatrace-buckets/#create-buckets","title":"Create Buckets","text":""},{"location":"6-dynatrace-buckets/#95-day-retention","title":"95 Day Retention","text":"<p>Click on the <code>+ Bucket</code> button to add a new bucket.  This bucket will store the application logs we want to retain for a longer duration, 95 days.</p> <p>Configure the new bucket as follows:</p> <p>Bucket name: <pre><code>observe_and_troubleshoot_apps_95_days\n</code></pre></p> <p>Display name: <pre><code>Observe and Troubleshoot Apps (95 Days)\n</code></pre></p> <p>Retention period (in days): <pre><code>95\n</code></pre></p> <p>Bucket table type: <pre><code>logs\n</code></pre></p> <p>Click on <code>Create</code> to add the new bucket.</p> <p></p> <p>It will take a minute or two for the new bucket to be generated.  Refresh the page until the bucket Status is <code>Active</code>.</p> <p></p> <p>Creating a new bucket has no effect on data storage in Grail.  Logs will continue to be stored in the existing buckets, including the default bucket.  We will use Dynatrace OpenPipeline in upcoming lab modules to assign log data to the new bucket.</p>"},{"location":"6-dynatrace-buckets/#365-day-retention","title":"365 Day Retention","text":"<p>Click on the <code>+ Bucket</code> button to add a new bucket.  This bucket will store the infrastructure logs that we are required to keep (theoretically) for an entire year.</p> <p>Configure the new bucket as follows:</p> <p>Bucket name: <pre><code>infrastructure_obs_ai_ops_365_days\n</code></pre></p> <p>Display name: <pre><code>Infrastructure Observability and AIOps (365 Days)\n</code></pre></p> <p>Retention period (in days): <pre><code>365\n</code></pre></p> <p>Bucket table type: <pre><code>logs\n</code></pre></p> <p>Click on <code>Create</code> to add the new bucket.</p> <p></p> <p>It will take a minute or two for the new bucket to be generated.  Refresh the page until the bucket Status is <code>Active</code>.</p> <p></p> <p>Creating a new bucket has no effect on data storage in Grail.  Logs will continue to be stored in the existing buckets, including the default bucket.  We will use Dynatrace OpenPipeline in upcoming lab modules to assign log data to the new bucket.</p>"},{"location":"6-dynatrace-buckets/#7-day-retention","title":"7 Day Retention","text":"<p>Click on the <code>+ Bucket</code> button to add a new bucket.  This bucket will store the various logs that we don't need to keep very long and can purge after a week.</p> <p>Configure the new bucket as follows:</p> <p>Bucket name: <pre><code>log_management_analytics_7_days\n</code></pre></p> <p>Display name: <pre><code>Log Management and Analytics (7 Days)\n</code></pre></p> <p>Retention period (in days): <pre><code>7\n</code></pre></p> <p>Bucket table type: <pre><code>logs\n</code></pre></p> <p>Click on <code>Create</code> to add the new bucket.</p> <p></p> <p>It will take a minute or two for the new bucket to be generated.  Refresh the page until the bucket Status is <code>Active</code>.</p> <p></p> <p>Creating a new bucket has no effect on data storage in Grail.  Logs will continue to be stored in the existing buckets, including the default bucket.  We will use Dynatrace OpenPipeline in upcoming lab modules to assign log data to the new bucket.</p>"},{"location":"6-dynatrace-buckets/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully created custom buckets for Grail storage management.</p> <ul> <li>Custom Logs Buckets<ul> <li>Logs for the <code>Observe and Troubleshoot Apps</code> use case, retained for 95 Days</li> <li>Logs for the <code>Infrastructure Observability and AIOps</code> use case, retained for 365 Days</li> <li>Logs for the <code>Log Management and Analytics</code> use case, retained for 7 Days</li> </ul> </li> </ul> <p>These retention periods were chosen arbitrarily for this lab, your retention periods in the real-world will vary.</p>"},{"location":"6-dynatrace-buckets/#continue","title":"Continue","text":"<ul> <li>Continue to Configuring OpenPipeline for Astronomy Shop logs</li> </ul>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/","title":"Dynatrace OpenPipeline - Astronomy Shop Logs","text":"<p>In this module we'll utilize Dynatrace OpenPipeline to process <code>astronomy-shop</code> application logs at ingest, in order to make them easier to analyze and leverage.  The logs will be ingested by OpenTelemetry Collector, deployed on Kubernetes as part of the previous module.  With OpenPipeline, the logs will be processed at ingest, to manipulate fields, extract metrics, and raise alert events in case of any issues.</p> <p>The OpenTelemetry Collector deployed as a Daemonset is collecting Pod logs from the Node's filesystem and shipping them to Dynatrace.  The application Pods from the Astronomy Shop application have been instrumented with the OpenTelemetry SDK.  The OpenTelemetry SDK is configured to ship logs (,traces, and metrics) to Dynatrace via the OpenTelemetry Collector deployed as a Deployment (Gateway).  Due to the differences in how these logs are collected, they do not contain the same metadata.  While these logs contain a lot of useful information, they are missing valuable fields/attributes that will make them easier to analyze in context.  These logs can be enriched at ingest, using OpenPipeline.  Additionally, OpenPipeline allows us to process fields, extract new data types, manage permissions, and modify storage retention.</p> <p>Goals:</p> <ul> <li>Add OpenTelemetry service name and namespace</li> <li>Enrich SDK logs with additional Kubernetes metadata</li> <li>Apply Dynatrace technology bundle (Java)</li> <li>Extract data: Payment transaction business event</li> <li>Extract metrics: Payment transaction amount</li> <li>Storage retention with bucket assignment</li> </ul> <p></p> <p>OpenPipeline is an architectural component of Dynatrace SaaS.  It resides between the Dynatrace SaaS tenant and Grail data lakehouse.  Logs (,traces, metrics, events, and more) are sent to the Dynatrace SaaS tenant and route through OpenPipeline where they are enriched, transformed, and contextualized prior to being stored in Grail.</p> <ul> <li>Learn More</li> </ul>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#query-logs","title":"Query Logs","text":"<p>Query and discover the Astronomy Shop logs as they are ingested and stored in Dynatrace.  Use Dynatrace Query Language (DQL) to transform the logs at query time and prepare for Dynatrace OpenPipeline configuration.</p> <p>Import Notebook into Dynatrace</p> <p>Download Astronomy Shop Logs Notebook</p> <p>Astronomy Shop Logs - Ondemand Processing at Query Time (Notebook)</p> <p>In OpenTelemetry, <code>service.name</code> and <code>service.namespace</code> are used to provide meaningful context about the services generating telemetry data:</p> <p><code>service.name</code>: This is the logical name of the service. It should be the same for all instances of a horizontally scaled service. For example, if you have a shopping cart service, you might name it shoppingcart.</p> <p><code>service.namespace</code>: This is used to group related services together. It helps distinguish a group of services that logically belong to the same system or team. For example, you might use Shop as the namespace for all services related to an online store.</p> <p>These attributes help in organizing and identifying telemetry data, making it easier to monitor and troubleshoot services within a complex system.</p> <p>The logs originating from the OpenTelemetry SDK contain both the <code>service.name</code> and <code>service.namespace</code>.  However, the Pod logs which contain stdout and stderr messages from the containers - do not.  In order to make it easier to analyze the log files and unify the telemetry, the <code>service.name</code> and <code>service.namespace</code> attributes should be added to the Pod logs with Dynatrace OpenPipeline.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-name","title":"OpenTelemetry Service Name","text":"<p>Query the <code>astronomy-shop</code> logs filtered on <code>isNull(service.name)</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\")\n| filter isNull(service.name) and isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, service.name, service.namespace\n</code></pre></p> <p></p> <p>The value for <code>service.name</code> can be obtained from multiple different fields, but based on the application configuration - it is best to use the value from <code>app.label.component</code>.</p> <p>Use DQL to transform the logs and apply the <code>service.name</code> value.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\")\n| filter isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd service.name = app.label.component\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, service.name, service.namespace\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-namespace","title":"OpenTelemetry Service Namespace","text":"<p>Query the <code>astronomy-shop</code> logs filtered on <code>isNull(service.namespace)</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter isNull(service.namespace) and isNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd service.name = app.label.component\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, app.annotation.service.namespace, service.name, service.namespace\n</code></pre></p> <p></p> <p>The Pods have been annotated with the service namespace.  The <code>k8sattributes</code> processor has been configured to add this annotation as an attribute, called <code>app.annotation.service.namespace</code>.  This field can be used to populate the <code>service.namespace</code>.</p> <p>Use DQL to transform the logs and apply the <code>service.namespace</code> value.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter isNull(service.namespace) and isNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd service.name = app.label.component\n| fieldsAdd service.namespace = app.annotation.service.namespace\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, app.annotation.service.namespace, service.name, service.namespace\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-sdk-logs","title":"OpenTelemetry SDK Logs","text":"<p>The logs generated and exported by the OpenTelemetry SDK are missing Kubernetes attributes, or in some cases have the wrong values set.  The OpenTelemetry SDK, unless specifically configured otherwise, is not aware of the Kubernetes context in which of the application runs.  As a result, when the OpenTelemetry Collector that's embedded in <code>astronomy-shop</code> sends the logs to the Dynatrace OpenTelemetry Collector via OTLP, the Kubernetes attributes are populated with the Kubernetes context of the <code>astronomy-shop-otelcol</code> workload.  This makes these attributes unreliable when analyzing logs.  In order to make it easier to analyze the log files and unify the telemetry, the Kubernetes attributes should be correct for the SDK logs with Dynatrace OpenPipeline.</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language</code> and <code>astronomy-shop-otelcol</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter isNotNull(telemetry.sdk.language) and matchesValue(k8s.deployment.name,\"astronomy-shop-otelcol\") and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, service.name, telemetry.sdk.language, k8s.namespace.name, k8s.deployment.name, k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n</code></pre></p> <p></p> <p>Potentially Resolved</p> <p>It is possible that in your environment this mapping of log data to the <code>astronomy-shop-otelcol</code> deployment is resolved.  If that is the case, then the query will not return any results.  That is OK.  We will still build out the OpenPipeline solution.</p> <p>The <code>k8s.namespace.name</code> is correct, however the <code>k8s.deployment.name</code>, <code>k8s.pod.name</code>, <code>k8s.pod.uid</code>, <code>k8s.replicaset.name</code>, and <code>k8s.node.name</code> are incorrect.  Since the <code>k8s.deployment.name</code> is based on the <code>service.name</code>, this field can be used to correct the <code>k8s.deployment.name</code> value.  The other values can be set to <code>null</code> in order to avoid confusion with the <code>astronomy-shop-otelcol</code> workload.</p> <p>Use DQL to transform the logs and set the <code>k8s.deployment.name</code> value while clearing the other fields.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter isNotNull(telemetry.sdk.language) and matchesValue(k8s.deployment.name,\"astronomy-shop-otelcol\") and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd k8s.deployment.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd k8s.container.name = service.name\n| fieldsAdd app.label.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd app.label.component = service.name\n| fieldsRemove k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n| fields timestamp, service.name, telemetry.sdk.language, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.name, app.label.component\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#java-technology-bundle","title":"Java Technology Bundle","text":"<p>Many applications written in a specific programming language will utilize known logging frameworks that have standard patterns, fields, and syntax for log messages.  For Java, these include frameworks such as Log4j, Logback, java.util.logging, etc.  Dynatrace OpenPipeline has a wide variety of Technology Processor Bundles, which can be easily added to a Pipeline to help format, clean up, and optimize logs for analysis.</p> <p>The Java technology processor bundle can be applied to the <code>astronomy-shop</code> logs that we know are originating from Java applications.</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language</code> or the <code>astronomy-shop-adservice</code> Java app.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter (matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false) or matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false)) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filter matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false) or matchesValue(k8s.deployment.name,\"astronomy-shop-kafka\", caseSensitive:false)\n| sort timestamp desc\n| limit 15\n| append [fetch logs\n          | filter matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false)\n          | fieldsAdd k8s.deployment.name = concat(k8s.namespace.name,\"-\",service.name)\n          | sort timestamp desc\n          | limit 15]\n| sort timestamp desc\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, telemetry.sdk.language, content\n</code></pre></p> <p></p> <p>These are the logs that will be modified using the Java technology processor bundle within OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transactions","title":"PaymentService Transactions","text":"<p>Most (if not all) applications and microservices drive business processes and outcomes.  Details about the execution of these business processes is often written out to the logs by the application.  Dynatrace OpenPipeline is able to extract this business-relevant information as a business event (bizevent).</p> <p>Log to Business Event Documentation</p> <p>DQL is fast and powerful, allowing us to query log files and summarize the data to generate timeseries for dashboards, alerts, AI-driven forecasting and more.  While it's handy to generate timeseries metric data from logs when we didn't know we would need it, it's better to generate timeseries metric data from logs at ingest for the use cases that we know ahead of time.  Dynatrace OpenPipeline is able to extract metric data from logs on ingest.</p> <p>Log to Metric Documentation</p> <p>The <code>paymentservice</code> component of <code>astronomy-shop</code> generates a log record every time it processes a payment transaction successfully.  This information is nested within a <code>JSON</code> structured log record, including the transactionId, amount, cardType, and currencyCode.  By parsing these relevant logs for the fields we need, Dynatrace OpenPipeline can be used to generate a payment transaction business event and a payment transaction amount metric on log record ingest.</p> <p>Query the <code>astronomy-shop</code> logs filtered on the <code>paymentservice</code> logs with a <code>trace_id</code> attribute.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, k8s.container.name, trace_id\n</code></pre></p> <p></p> <p>The <code>content</code> field is structured <code>JSON</code>.  The parse command can be used to parse the JSON content and add the fields we need for our use case.</p> <pre><code>{\n  \"level\": \"30\",\n  \"time\": \"1742928663142\",\n  \"pid\": \"24\",\n  \"hostname\": \"astronomy-shop-paymentservice-6fb4c9ff9b-t45xn\",\n  \"trace_id\": \"f3c6358fe776c7053d0fd2dab7bc470f\",\n  \"span_id\": \"880430306f41a648\",\n  \"trace_flags\": \"01\",\n  \"transactionId\": \"c54b6b4c-ebf1-4191-af21-5f583d0d0c87\",\n  \"cardType\": \"visa\",\n  \"lastFourDigits\": \"5647\",\n  \"amount\": {\n    \"units\": {\n      \"low\": \"37548\",\n      \"high\": \"0\",\n      \"unsigned\": false\n    },\n    \"nanos\": \"749999995\",\n    \"currencyCode\": \"USD\"\n  },\n  \"msg\": \"Transaction complete.\"\n}\n</code></pre> <p>Use DQL to transform the logs and parse the payment fields from the JSON content.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, k8s.container.name, trace_id\n| parse content, \"JSON:json_content\"\n| fieldsAdd app.payment.msg = json_content[`msg`]\n| filter app.payment.msg == \"Transaction complete.\"\n| fieldsAdd app.payment.cardType = json_content[`cardType`]\n| fieldsAdd app.payment.amount = json_content[`amount`][`units`][`low`]\n| fieldsAdd app.payment.currencyCode = json_content[`amount`][`currencyCode`]\n| fieldsAdd app.payment.transactionId = json_content[`transactionId`]\n| fieldsRemove json_content\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, next.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#configure-openpipeline","title":"Configure OpenPipeline","text":"<p>Configure Dynatrace OpenPipeline for Astronomy Shop logs.</p> <p>View Images</p> <p>If the images are too small and the text is difficult to read, right-click and open the image in a new tab.</p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p> <p>In your Dynatrace tenant, launch the (new) <code>Settings</code> app.  From the <code>Process and contextualize</code> menu, click on <code>OpenPipeline</code>.</p> <p></p> <p>OpenPipeline App</p> <p>Depending on your Dynatrace tenant version, you may need to use the OpenPipeline app instead.</p> <p>Begin by selecting <code>Logs</code> from the menu of telemetry types.  Then choose <code>Pipelines</code>.  Click on <code>+ Pipeline</code> to add a new pipeline.</p> <p></p> <p>Name the new pipeline, <code>Astronomy Shop OpenTelemetry Logs</code>.  Click on the <code>Processing</code> tab to begin adding <code>Processor</code> rules.</p> <p></p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-name_1","title":"OpenTelemetry Service Name","text":"<p>Add a processor to set the OpenTelemetry Service Name.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry Service Name\n</code></pre></p> <p>Matching condition: <pre><code>isNull(service.name) and isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd service.name = app.label.component\n</code></pre></p> <p></p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-namespace_1","title":"OpenTelemetry Service Namespace","text":"<p>Add a processor to set the OpenTelemetry Service Namespace.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry Service Namespace\n</code></pre></p> <p>Matching condition: <pre><code>isNull(service.namespace) and isNotNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd service.namespace = app.annotation.service.namespace\n</code></pre></p> <p></p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-sdk-logs_1","title":"OpenTelemetry SDK Logs","text":"<p>Add a processor to transform the OpenTelemetry SDK Logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry SDK Logs\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(telemetry.sdk.language) and matchesValue(k8s.deployment.name,\"astronomy-shop-otelcol\") and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd k8s.deployment.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd k8s.container.name = service.name\n| fieldsAdd app.label.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd app.label.component = service.name\n| fieldsRemove k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n</code></pre></p> <p></p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#java-technology-bundle_1","title":"Java Technology Bundle","text":"<p>Add a processor to enrich the Java logs using the Java Technology Bundle.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Technology Bundle &gt; Java\n</code></pre></p> <p>Matching condition: <pre><code>(matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false) or matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false)) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p></p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transactions_1","title":"PaymentService Transactions","text":"<p>Add a processor to parse the PaymentService Transaction logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>PaymentService Transactions\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(service.name,\"paymentservice\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n</code></pre></p> <p>Processor definition: <pre><code>parse content, \"JSON:json_content\"\n| fieldsAdd app.payment.msg = json_content[`msg`]\n| fieldsAdd app.payment.cardType = json_content[`cardType`]\n| fieldsAdd app.payment.amount = json_content[`amount`][`units`][`low`]\n| fieldsAdd app.payment.currencyCode = json_content[`amount`][`currencyCode`]\n| fieldsAdd app.payment.transactionId = json_content[`transactionId`]\n| fieldsRemove json_content\n</code></pre></p> <p></p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transaction-bizevent","title":"PaymentService Transaction BizEvent","text":"<p>Switch to the <code>Data extraction</code> tab.</p> <p>Add a processor to extract a <code>Business Event</code>.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Business Event\n</code></pre></p> <p>Name: <pre><code>PaymentService Transaction\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(app.payment.cardType) and isNotNull(app.payment.amount) and isNotNull(app.payment.currencyCode) and isNotNull(app.payment.transactionId)\n</code></pre></p> <p>Event type: <pre><code>Static String : astronomy-shop.app.payment.complete\n</code></pre></p> <p>Event provider: <pre><code>Static String: astronomy-shop.opentelemetry\n</code></pre></p> <p>Field Extraction:</p> Fields app.payment.msg app.payment.cardType app.payment.amount app.payment.currencyCode app.payment.transactionid log.file.path trace_id span_id service.namespace service.name k8s.namespace.name k8s.container.name k8s.pod.name k8s.pod.uid <p></p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transaction-metric","title":"PaymentService Transaction Metric","text":"<p>Switch to the <code>Metric Extraction</code> tab.</p> <p>Add a processor to set extract a metric from the PaymentService Transaction logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Value metric\n</code></pre></p> <p>Name: <pre><code>PaymentService Transaction\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(app.payment.cardType) and isNotNull(app.payment.amount) and isNotNull(app.payment.currencyCode) and isNotNull(app.payment.transactionId)\n</code></pre></p> <p>Field extraction: <pre><code>app.payment.amount\n</code></pre></p> <p>Metric key: <pre><code>otel.astronomy-shop.app.payment.amount\n</code></pre></p> <p>Dimensions:</p> Field Dimension app.payment.cardType cardType app.payment.currencyCode currencyCode <p></p> <p>Consider Saving</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#storage","title":"Storage","text":"<p>Switch to the <code>Storage</code> tab.</p> <p>Add a processor to set the bucket assignment.  Click on <code>+ Processor</code> to add a new  Bucket Assignment processor.</p> <p>Name: <pre><code>Observe and Troubleshoot Apps Bucket\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(status,\"INFO\") or matchesValue(status,\"WARN\") or matchesValue(status,\"ERROR\")\n</code></pre></p> <p>Storage: <pre><code>Observe and Troubleshoot Apps (95 Days)\n</code></pre></p> <p></p> <p>This will result in Astronomy Shop logs with <code>INFO</code>, <code>WARN</code>, or <code>ERROR</code> status values and matching this pipeline to be stored for 95 days in this bucket.</p> <p>Add a processor to set the bucket assignment.  Click on <code>+ Processor</code> to add a new  Bucket Assignment processor.</p> <p>Name: <pre><code>Log Management and Analytics Bucket\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(status,\"NONE\")\n</code></pre></p> <p>Storage: <pre><code>Log Management and Analytics (7 Days)\n</code></pre></p> <p></p> <p>This will result in Astronomy Shop logs with <code>NONE</code> status values and matching this pipeline to be stored for 7 days in this bucket.</p> <p>The pipeline is now configured, click on <code>Save</code> to save the pipeline configuration.</p> <p></p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#dynamic-route","title":"Dynamic Route","text":"<p>A pipeline will not have any effect unless logs are configured to be routed to the pipeline.  With dynamic routing, data is routed based on a matching condition. The matching condition is a DQL query that defines the data set you want to route.</p> <p>Click on <code>Dynamic Routing</code> to configure a route to the target pipeline.  Click on <code>+ Dynamic Route</code> to add a new route.</p> <p></p> <p>Configure the <code>Dynamic Route</code> to use the <code>Astronomy Shop OpenTelemetry Logs</code> pipeline.</p> <p>Name: <pre><code>Astronomy Shop OpenTelemetry Logs\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\") and isNull(event.domain)\n</code></pre></p> <p>Pipeline: <pre><code>Astronomy Shop OpenTelemetry Logs\n</code></pre></p> <p>Click <code>Add</code> to add the route.</p> <p></p> <p>Validate that the route is enabled in the <code>Status</code> column.  Click on <code>Save</code> to save the dynamic route table configuration.</p> <p></p> <p>Allow <code>astronomy-shop</code> to generate new log data that will be routed through the new pipeline (3-5 minutes).</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#analyze-results","title":"Analyze Results","text":"<p>Analyze the Astronomy Shop logs after Dynatrace OpenPipeline processing.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#analyze-the-results-in-dynatrace-notebook","title":"Analyze the results in Dynatrace (Notebook)","text":"<p>Use the Notebook from earlier to analyze the results.</p> <p>OpenTelemetry Service Name</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>isNotNull(service.name)</code> to analyze with <code>OpenTelemetry Service Name</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\")\n| filter isNotNull(service.name) and isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, service.name, service.namespace\n</code></pre></p> <p></p> <p>OpenTelemetry Service Namespace</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>isNotNull(service.namespace)</code> to analyze with <code>OpenTelemetry Service Namespace</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter isNotNull(service.namespace) and isNotNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, app.annotation.service.namespace, service.name, service.namespace\n</code></pre></p> <p></p> <p>OpenTelemetry SDK Logs</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language</code> to analyze with <code>OpenTelemetry SDK Logs</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter isNotNull(telemetry.sdk.language) and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, service.name, telemetry.sdk.language, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.name, app.label.component, k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n</code></pre></p> <p></p> <p>Java Technology Bundle</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language == \"java\"</code> to analyze with <code>Java Technology Bundle</code> logs.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter (matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false) or matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false)) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 50\n</code></pre></p> <p></p> <p>You likely won't notice anything different about these logs.  This exercise was meant to show you how to use the technology bundles.</p> <p>PaymentService Transactions</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>service.name == \"paymentservice\"</code> to analyze with <code>PaymentService</code> logs.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and isNotNull(service.name)\n| filterOut event.domain == \"k8s\"\n| filter matchesValue(service.name,\"paymentservice\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, k8s.container.name, trace_id, app.payment.msg, app.payment.cardType, app.payment.amount, app.payment.currencyCode, app.payment.transactionId\n</code></pre></p> <p></p> <p>Query the <code>PaymentService</code> Business Events.</p> <p>DQL: PaymentService Transaction Business Events <pre><code>fetch bizevents\n| filter matchesValue(event.type,\"astronomy-shop.app.payment.complete\")\n| sort timestamp desc\n| limit 10\n</code></pre></p> <p></p> <p>Query the <code>PaymentService</code> Metric.</p> <p>DQL: PaymentService Transaction Extracted Metric <pre><code>timeseries sum(`log.otel.astronomy-shop.app.payment.amount`), by: { currencyCode, cardType }\n| fieldsAdd value.A = arrayAvg(`sum(\\`log.otel.astronomy-shop.app.payment.amount\\`)`)\n</code></pre></p> <p></p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#business-data-in-context","title":"Business Data in Context","text":"<p>Extracting business data from logs has many benefits.  By generating a bizevent with OpenPipeline, the business data becomes immediately accessible for analysis, forensics, trending, and alerting - business observability.  Business events can be configured with retention times exceeding what you might choose for log retention.  Most importantly, bizevents enable business data to be captured in full context of topology, applications, infrastructure, and observability signals.  Let's explore the value of business data in context.</p> <p>Business Data</p> <p>In this example, the payment transaction details are captured on the extracted bizevent.  We can query and analyze the business data to measure business KPI and business outcomes.</p> <p></p> <p>Business Data in Kubernetes Context</p> <p>By including the Kubernetes attributes on the bizevent, this allows us to understand the topology and infrastructure that drives the business outcomes related to these payments.  Any anomalies that may occur impacting business KPIs or infrastructure health will be directly correlated, in context.</p> <p></p> <p>Business Data in Log Context</p> <p>By including a small subset of log attributes on the bizevent, this allows us to analyze diagnostic logs surrounding the payment transaction.</p> <p></p> <p>We can quickly drill down into the logs, including the surrounding logs, for each of the payment transaction bizevents.</p> <p></p> <p>Business Data in Trace Context</p> <p>By including the distributed tracing attributes on the bizevent, this allows us to analyze the distributed traces and spans for the payment transaction.</p> <p></p> <p>We can quickly drill down into the distributed trace to see everything that happened end-to-end for the payment transaction, including upstream/downstream dependencies, input parameters, and code-level details.</p> <p></p> <p>Dynatrace enables you to understand the health of business outcomes and business processes and directly tie their health to the underlying applications and infrastructure that make them possible - even with OpenTelemetry.</p>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully set up Dynatrace OpenPipeline pipelines to process the Astronomy Shop logs at ingest.</p> <ul> <li>Astronomy Shop logs<ul> <li>Add OpenTelemetry service name and namespace fields to unify telemetry signals and enable out-of-the-box analysis</li> <li>Enrich SDK logs with additional Kubernetes metadata to unify telemetry signals and analyze Kubernetes context</li> <li>Apply Dynatrace technology bundle (Java) to transform logs based on known Java standards and frameworks</li> <li>Extract data: Payment transaction business event to measure business outcomes and link them to system health</li> <li>Extract metrics: Payment transaction amount to measure business KPIs and link them to system health</li> <li>Routed logs to a specific bucket in Grail based on retention period needs</li> <li>Analyzed business data in context of topology, applications, infrastructure, and observability signals</li> </ul> </li> </ul>"},{"location":"7-dynatrace-openpipeline-astronomy-shop-logs/#continue","title":"Continue","text":"<ul> <li>Continue to Configure Dynatrace OpenPipeline for Kubernetes Events</li> </ul>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/","title":"Dynatrace OpenPipeline - Kubernetes Events","text":"<p>In this module we'll utilize Dynatrace OpenPipeline to process Kubernetes Events at ingest, in order to make them easier to analyze and leverage.  The events will be ingested by OpenTelemetry Collector as logs, deployed on Kubernetes as part of the previous module.  With OpenPipeline, the logs will be processed at ingest, to manipulate fields, extract metrics, and raise alert events in case of any issues.</p> <p>The OpenTelemetry Collector, specifically the Contrib Distro running as a Deployment, is configured to capture Kubernetes Events using the <code>k8s_objects</code> receiver.  These events are shipped to Dynatrace as OpenTelemetry logs.  While these logs contain a lot of useful information, they are missing valuable fields/attributes that will make them easier to analyze in context.  These logs can be enriched at ingest, using OpenPipeline.  Additionally, OpenPipeline allows us to process fields, extract new data types, manage permissions, and modify storage retention.</p> <p>Goals:</p> <ul> <li>Enrich logs with additional Kubernetes metadata</li> <li>Enrich the log message, via the content field</li> <li>Set loglevel and status fields</li> <li>Remove unwanted fields/attributes</li> <li>Add OpenTelemetry service name and namespace</li> <li>Extract metrics: event count</li> <li>Storage retention with bucket assignment</li> </ul> <p></p> <p>OpenPipeline is an architectural component of Dynatrace SaaS.  It resides between the Dynatrace SaaS tenant and Grail data lakehouse.  Logs (,traces, metrics, events, and more) are sent to the Dynatrace SaaS tenant and route through OpenPipeline where they are enriched, transformed, and contextualized prior to being stored in Grail.</p> <ul> <li>Learn More</li> </ul>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#generate-kubernetes-events","title":"Generate Kubernetes Events","text":"<p>Kubernetes Events will only be generated when Kubernetes orchestration causes changes within the environment.  Generate new Kubernetes Events for analysis prior to continuing.</p> <p>Command: <pre><code>kubectl delete pods -n astronomy-shop --field-selector=\"status.phase=Running\"\n</code></pre></p> <p>This will delete all running pods for <code>astronomy-shop</code> and schedule new ones, resulting in many new Kubernetes Events.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#query-logs","title":"Query Logs","text":"<p>Query and discover the Kubernetes Events logs as they are ingested and stored in Dynatrace.  Use Dynatrace Query Language (DQL) to transform the logs at query time and prepare for Dynatrace OpenPipeline configuration.</p> <p>Import Notebook into Dynatrace</p> <p>Download Kubernetes Events Logs Notebook</p> <p>Kubernetes Events - Ondemand Processing at Query Time (Notebook)</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#kubernetes-attributes","title":"Kubernetes Attributes","text":"<p>When the OpenTelemetry Collector captures Kubernetes Events using the <code>k8s_objects</code> receiver, most of the Kubernetes context information is stored in fields with the prefix <code>object.*</code> and <code>object.involvedObject.*</code>.  These fields aren't used in other logs related to Kubernetes observability.  Dynatrace OpenPipeline enables us to parse these object fields and use them to populate the normal Kubernetes (<code>k8s.*</code>) attributes.</p> <p>Field Name Casing</p> <p>In some cases, it has been observed that the fields that should start with <code>object.involvedObject.*</code> are instead starting with <code>object.involvedobject.*</code>.  When using field names with DQL, the proper case needs to be used.  If you encounter this, please match the casing you observe in your environment.</p> <p>Query the Kubernetes logs filtered on <code>event.domain == \"k8s\"</code> and <code>telemetry.sdk.name</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| sort timestamp desc\n| limit 25\n</code></pre></p> <p></p> <p>Notice the many fields with the <code>object.*</code> prefix that provide valuable context information about the Kubernetes component related to the event.  Use the <code>object.involvedObject.namespace</code>, <code>object.involvedObject.kind</code>, and <code>object.involvedObject.name</code> fields to set the Kubernetes (<code>k8s.*</code>) attributes.</p> <p>Use DQL to transform the logs and apply the <code>k8s.*</code> attributes.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| sort timestamp desc\n| limit 25\n| fieldsAdd k8s.namespace.name = object.involvedObject.namespace\n| fieldsAdd k8s.pod.name = if(object.involvedObject.kind == \"Pod\",object.involvedObject.name)\n| fieldsAdd k8s.deployment.name = if(object.involvedObject.kind == \"Deployment\",object.involvedObject.name)\n| fieldsAdd k8s.replicaset.name = if(object.involvedObject.kind == \"ReplicaSet\",object.involvedObject.name)\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>These changes with DQL allow us to populate the relevant Kubernetes attributes where we know the correct value.  For example, if the involved object is a Deployment, then we can set the <code>k8s.deployment.name</code> attribute.  In order to populate the missing fields, we can apply logic and DQL parsing commands.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#kubernetes-replicaset","title":"Kubernetes ReplicaSet","text":"<p>For the Kubernetes Events that impact a ReplicaSet, we need to set the <code>k8s.replicaset.name</code> and <code>k8s.deployment.name</code>.  Since the event doesn't directly impact a Pod and we don't know the Pod unique id, the <code>k8s.pod.name</code> attribute should remain <code>null</code>.</p> <p>Query the Kubernetes logs filtered on <code>object.involvedObject.kind == \"ReplicaSet\"</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"ReplicaSet\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>The ReplicaSet name follows the naming convention <code>&lt;deployment-name&gt;-&lt;replicaset-hash&gt;</code>.  Use DQL to transform the logs, parse the ReplicaSet name, and apply the value to the <code>k8s.deployment.name</code> attribute.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"ReplicaSet\")\n| sort timestamp desc\n| limit 25\n| parse object.involvedObject.name, \"LD:deployment ('-' ALNUM:hash EOS)\"\n| fieldsAdd k8s.deployment.name = deployment\n| fieldsAdd k8s.replicaset.name = object.involvedobject.name\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#kubernetes-pod","title":"Kubernetes Pod","text":"<p>For the Kubernetes Events that impact a Pod, we need to set the <code>k8s.pod.name</code>, <code>k8s.replicaset.name</code> and <code>k8s.deployment.name</code> since we know all (3).</p> <p>Query the Kubernetes logs filtered on <code>object.involvedObject.kind == \"Pod\"</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"Pod\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>The Pod name follows the naming convention <code>&lt;deployment-name&gt;-&lt;replicaset-hash&gt;-&lt;pod-hash&gt;</code>.  Use DQL to transform the logs, parse the Pod name, and apply the value ot the <code>k8s.deployment.name</code> and <code>k8s.replicaset.name</code> attributes.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"Pod\")\n| sort timestamp desc\n| limit 25\n| parse object.involvedObject.name, \"LD:deployment ('-' ALNUM:hash '-' ALNUM:unique EOS)\"\n| fieldsAdd k8s.deployment.name = deployment\n| fieldsAdd k8s.replicaset.name = concat(deployment,\"-\",hash)\n| fieldsAdd k8s.pod.name = object.involvedObject.name\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#content-field-and-drop-fields","title":"Content Field and Drop Fields","text":"<p>The <code>content</code> field is a standard semantic attribute/field for log data.  Best practice is to have a populated content field, as the minimum fields necessary for log analysis are timestamp and content.  For the Kubernetes Events, the content field is null.  There are other fields on the logs that can be used to populate the content field, <code>object.reason</code> and <code>object.message</code> are the best candidates.</p> <p>Additionally, there are several fields with the <code>object.metadata.*</code> prefix which provide little to no value.  These fields add log bloat, consuming unnecessary storage and increasing query response times (albeit negligbly).</p> <p>Query the Kubernetes logs focused on these attributes.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, object.reason, object.message, object.metadata.managedfields, object.metadata.name, object.metadata.uid\n</code></pre></p> <p></p> <p>We can use the <code>object.reason</code> and <code>object.message</code> fields together to create a valuable <code>content</code> field.  The <code>object.metadata.managedfields</code>, <code>object.metadata.name</code>, and <code>object.metadata.uid</code> fields are redudant or useless, they can be removed.</p> <p>Use DQL to transform the logs, set the <code>content</code> field and remove the useless fields.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| filter matchesValue(content,\"\") or matchesValue(content,\" \") or isNull(content)\n| sort timestamp desc\n| limit 25\n| fieldsAdd content = if(isNull(object.reason), object.message, else:concat(object.reason,\": \", object.message))\n| fieldsAdd object.metadata.uid = null, object.metadata.name = null, object.metadata.managedfields = null\n| fields timestamp, content, object.reason, object.message, object.metadata.managedfields, object.metadata.name, object.metadata.uid\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#opentelemetry-service-name-and-namespace","title":"OpenTelemetry Service Name and Namespace","text":"<p>In OpenTelemetry, <code>service.name</code> and <code>service.namespace</code> are used to provide meaningful context about the services generating telemetry data:</p> <p><code>service.name</code>: This is the logical name of the service. It should be the same for all instances of a horizontally scaled service. For example, if you have a shopping cart service, you might name it shoppingcart.</p> <p><code>service.namespace</code>: This is used to group related services together. It helps distinguish a group of services that logically belong to the same system or team. For example, you might use Shop as the namespace for all services related to an online store.</p> <p>These attributes help in organizing and identifying telemetry data, making it easier to monitor and troubleshoot services within a complex system.</p> <p>The logs for the Kubernetes Events do not include these fields.   In order to make it easier to analyze the log files and unify the telemetry, the <code>service.name</code> and <code>service.namespace</code> attributes should be added with Dynatrace OpenPipeline.</p> <p>Query the Kubernetes logs for <code>astronomy-shop</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, service.name, service.namespace, object.involvedObject.name\n</code></pre></p> <p></p> <p>The <code>k8s.deployment.name</code> can be split to obtain the <code>service.name</code> field.  Unfortunately, the <code>service.namespace</code> value does not exist anywhere on the event.  This value will need to be set as a static string.  Use the value that you set in the <code>$NAME</code> variable earlier, in the form <code>&lt;INITIALS&gt;-k8s-otel-o11y</code>.</p> <p>Use DQL to transform the logs, set the <code>service.name</code> and <code>service.namespace</code> fields.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(object.involvedObject.kind,\"Deployment\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd k8s.deployment.name = object.involvedObject.name\n| fieldsAdd split_deployment_name = splitString(k8s.deployment.name,k8s.namespace.name)\n| parse split_deployment_name[1], \"(PUNCT?) WORD:service.name\"\n| fieldsRemove split_deployment_name\n| fieldsAdd service.namespace = \"&lt;INITIALS&gt;-k8s-otel-o11y\"\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, service.name, service.namespace\n</code></pre> Be sure to replace <code>&lt;INITIALS&gt;</code> with the correct value in your query!</p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, next.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#configure-openpipeline","title":"Configure OpenPipeline","text":"<p>Configure Dynatrace OpenPipeline for Kubernetes Events logs.</p> <p>View Images</p> <p>If the images are too small and the text is difficult to read, right-click and open the image in a new tab.</p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p> <p>In your Dynatrace tenant, launch the (new) <code>Settings</code> app.  From the <code>Process and contextualize</code> menu, click on <code>OpenPipeline</code>.</p> <p></p> <p>OpenPipeline App</p> <p>Depending on your Dynatrace tenant version, you may need to use the OpenPipeline app instead.</p> <p>Begin by selecting <code>Logs</code> from the menu of telemetry types.  Then choose <code>Pipelines</code>.  Click on <code>+ Pipeline</code> to add a new pipeline.</p> <p></p> <p>Name the new pipeline, <code>OpenTelemetry Kubernetes Events</code>.  Click on the <code>Processing</code> tab to begin adding <code>Processor</code> rules.</p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#kubernetes-attributes_1","title":"Kubernetes Attributes","text":"<p>Add a processor to set the Kubernetes Attributes.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>Kubernetes Attributes\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd k8s.namespace.name = object.involvedObject.namespace\n| fieldsAdd k8s.pod.name = if(object.involvedObject.kind == \"Pod\",object.involvedObject.name)\n| fieldsAdd k8s.deployment.name = if(object.involvedObject.kind == \"Deployment\",object.involvedObject.name)\n| fieldsAdd k8s.replicaset.name = if(object.involvedObject.kind == \"ReplicaSet\",object.involvedObject.name)\n</code></pre></p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#kubernetes-replicaset_1","title":"Kubernetes ReplicaSet","text":"<p>Add a processor to set the values for Kubernetes ReplicaSet.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>Kubernetes ReplicaSet\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"ReplicaSet\")\n</code></pre></p> <p>Processor definition: <pre><code>parse object.involvedObject.name, \"LD:deployment ('-' ALNUM:hash EOS)\"\n| fieldsAdd k8s.deployment.name = deployment\n| fieldsRemove deployment, hash\n</code></pre></p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#kubernetes-pod_1","title":"Kubernetes Pod","text":"<p>Add a processor to set the values for Kubernetes Pod.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>Kubernetes Pod\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"Pod\")\n</code></pre></p> <p>Processor definition: <pre><code>parse object.involvedObject.name, \"LD:deployment ('-' ALNUM:hash '-' ALNUM:unique EOS)\"\n| fieldsAdd k8s.deployment.name = deployment\n| fieldsAdd k8s.replicaset.name = concat(deployment,\"-\",hash)\n| fieldsRemove deployment, hash, unique\n</code></pre></p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#loglevel-and-status","title":"Loglevel and Status","text":"<p>Add a processor to set the Loglevel and Status fields.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>Loglevel and Status\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(object.type) and (isNull(loglevel) or matchesValue(loglevel,\"NONE\")) and (isNull(status) or matchesValue(status,\"NONE\"))\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd loglevel = if(matchesValue(object.type,\"Normal\"),\"INFO\", else: if(matchesValue(object.type,\"Warning\"),\"WARN\", else: \"NONE\"))\n| fieldsAdd status = if(matchesValue(object.type,\"Normal\"),\"INFO\", else: if(matchesValue(object.type,\"Warning\"),\"WARN\", else: \"NONE\"))\n</code></pre></p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#content-field","title":"Content Field","text":"<p>Add a processor to set the content field.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>Content Field\n</code></pre></p> <p>Matching condition: <pre><code>(matchesValue(content,\"\") or matchesValue(content,\" \") or isNull(content)) and isNotNull(object.message)\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd content = if(isNull(object.reason), object.message, else:concat(object.reason,\": \", object.message))\n</code></pre></p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#remove-fields","title":"Remove Fields","text":"<p>Add a processor to drop the redudant and unnecessary.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Remove Fields\n</code></pre></p> <p>Name: <pre><code>Drop Fields\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n</code></pre></p> <p>Remove fields:</p> Fields object.metadata.name object.metadata.uid object.metadata.managedFields <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#opentelemetry-service-name","title":"OpenTelemetry Service Name","text":"<p>Add a processor to set the OpenTelemetry Service Name.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry Service Name\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.namespace.name,\"astronomy-shop\") and isNotNull(k8s.deployment.name) and isNull(service.name)\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd split_deployment_name = splitString(k8s.deployment.name,k8s.namespace.name)\n| parse split_deployment_name[1], \"(PUNCT?) WORD:service.name\"\n| fieldsRemove split_deployment_name\n</code></pre></p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#opentelemetry-service-namespace","title":"OpenTelemetry Service Namespace","text":"<p>Add a processor to set the OpenTelemetry Service Namespace.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Add Fields\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry Service Namespace\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.namespace.name,\"astronomy-shop\") and isNotNull(k8s.deployment.name) and isNotNull(service.name) and isNull(service.namespace)\n</code></pre></p> <p>Add fields:</p> Field Value service.namespace INITIALS-k8s-otel-o11y <p>Be sure to use the same <code>service.namespace</code> value that you have used elsewhere in this lab!</p> <p></p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#kubernetes-event-count","title":"Kubernetes Event Count","text":"<p>Switch to the <code>Metric Extraction</code> tab.</p> <p>Add a processor to set extract a metric from the Kubernetes event logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Counter metric\n</code></pre></p> <p>Name: <pre><code>Kubernetes Event Count\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(k8s.cluster.name) and isNotNull(k8s.namespace.name) and isNotNull(status)\n</code></pre></p> <p>Metric key: <pre><code>otel.k8s.event_count\n</code></pre></p> <p>Dimensions:</p> Fields k8s.namespace.name k8s.cluster.name status service.name <p></p> <p>Consider Saving</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#storage","title":"Storage","text":"<p>Switch to the <code>Storage</code> tab.</p> <p>Add a processor to set the bucket assignment.  Click on <code>+ Processor</code> to add a new  Bucket Assignment processor.</p> <p>Name: <pre><code>Observe and Troubleshoot Apps Bucket\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(status,\"WARN\")\n</code></pre></p> <p>Storage: <pre><code>Observe and Troubleshoot Apps (95 Days)\n</code></pre></p> <p></p> <p>This will result in Kubernetes Events logs with <code>WARN</code> status values that involve the <code>astronomy-shop</code> namespace matching this pipeline to be stored for 95 days in this bucket.</p> <p>Add a processor to set the bucket assignment.  Click on <code>+ Processor</code> to add a new  Bucket Assignment processor.</p> <p>Name: <pre><code>Infrastructure Observability and AIOps Bucket\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(status) and isNotNull(k8s.namespace.name)\n</code></pre></p> <p>Storage: <pre><code>Infrastructure Observability and AIOps (365 Days)\n</code></pre></p> <p></p> <p>This will result in any Kubernetes Events logs with a <code>status</code> field and <code>k8s.namespace.name</code> field matching this pipeline to be stored for 365 days in this bucket, if they did not match the previous processor.  Logs, or any other record type, processed through OpenPipeline can only be stored in one bucket.  The first matching processor is used to set the storage location.  </p> <p>The pipeline is now configured, click on <code>Save</code> to save the pipeline configuration.</p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#dynamic-route","title":"Dynamic Route","text":"<p>A pipeline will not have any effect unless logs are configured to be routed to the pipeline.  With dynamic routing, data is routed based on a matching condition. The matching condition is a DQL query that defines the data set you want to route.</p> <p>Click on <code>Dynamic Routing</code> to configure a route to the target pipeline.  Click on <code>+ Dynamic Route</code> to add a new route.</p> <p></p> <p>Configure the <code>Dynamic Route</code> to use the <code>OpenTelemetry Kubernetes Events</code> pipeline.</p> <p>Name: <pre><code>OpenTelemetry Kubernetes Events\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n</code></pre></p> <p>Pipeline: <pre><code>OpenTelemetry Kubernetes Events\n</code></pre></p> <p>Click <code>Add</code> to add the route.</p> <p></p> <p>Validate that the route is enabled in the <code>Status</code> column.  Click on <code>Save</code> to save the dynamic route table configuration.</p> <p></p> <p>Changes will typically take effect within a couple of minutes.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#analyze-results","title":"Analyze Results","text":"<p>Analyze the Kubernetes Events logs after Dynatrace OpenPipeline processing.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#generate-kubernetes-events_1","title":"Generate Kubernetes Events","text":"<p>Kubernetes Events will only be generated when Kubernetes orchestration causes changes within the environment.  Generate new Kubernetes Events for analysis prior to continuing.</p> <p>Command: <pre><code>kubectl delete pods -n astronomy-shop --field-selector=\"status.phase=Running\"\n</code></pre></p> <p>This will delete all running pods for <code>astronomy-shop</code> and schedule new ones, resulting in many new Kubernetes Events.</p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#analyze-the-results-in-dynatrace-notebook","title":"Analyze the results in Dynatrace (Notebook)","text":"<p>Use the Notebook from earlier to analyze the results.</p> <p>Kubernetes Attributes</p> <p>Query the Kubernetes Events logs fitered on <code>event.domain == \"k8s\"</code> to analyze with <code>Kubernetes Attributes</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>Kubernetes ReplicaSet</p> <p>Query the Kubernetes Events logs fitered on <code>object.involvedObject.kind == \"ReplicaSet\"</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"ReplicaSet\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>Kubernetes Pod</p> <p>Query the Kubernetes Events logs fitered on <code>object.involvedObject.kind == \"Pod\"</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.name) and matchesValue(object.involvedObject.kind,\"Pod\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.replicaset.name, k8s.pod.name, object.involvedObject.kind, object.involvedObject.name\n</code></pre></p> <p></p> <p>Content Field and Drop Fields</p> <p>Query the Kubernetes Events logs to view the new <code>content</code> field.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, object.reason, object.message, object.metadata.managedfields, object.metadata.name, object.metadata.uid\n</code></pre></p> <p></p> <p>OpenTelemetry Service Name and Namespace</p> <p>Query the Kubernetes Events logs filtered on <code>service.name</code> and <code>service.namespace</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(telemetry.sdk.name,\"opentelemetry\") and matchesValue(event.domain,\"k8s\") and matchesValue(k8s.resource.name,\"events\")\n| filter isNotNull(object.involvedObject.namespace) and isNotNull(object.involvedObject.kind) and isNotNull(object.involvedObject.name)\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and isNotNull(k8s.deployment.name) and isNotNull(service.name)\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, service.name, service.namespace, content\n</code></pre></p> <p></p> <p>Kubernetes Event Count Metric</p> <p>The Metric Extraction capability of OpenPipeline allows you to register a data point for a timeseries metric for each processed log record.  This data point can be a value extracted from the log record or it can simply increment a count for that metric.  In this example, we extract a count metric for every Kubernetes Event log record that matches the conditions.  This allows us to quickly query this information for alerting, trending, and automations without relying on heavy log data queries.</p> <p>DQL: After OpenPipeline <pre><code>timeseries sum(log.otel.k8s.event_count), by: {k8s.namespace.name, service.name, status}\n</code></pre></p> <p></p>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully set up Dynatrace OpenPipeline pipelines to process the Kubernetes Events logs at ingest.</p> <ul> <li>Kubernetes Events logs<ul> <li>Enrich logs with additional Kubernetes metadata to unify telemetry signals and analyze Kubernetes context</li> <li>Enrich the content field to fit logging standards and semantics</li> <li>Set loglevel and status fields to easily identify errors with Kubernetes context</li> <li>Remove unwanted fields/attributes to reduce log bloat and optimize queries</li> <li>Add OpenTelemetry service name and namespace to unify telemetry signals and enable out-of-the-box analysis</li> <li>Extract metrics: event count to track Kubernetes health and reduce heavy log queries</li> <li>Routed logs to a specific bucket in Grail based on retention period needs</li> </ul> </li> </ul>"},{"location":"8-dynatrace-openpipeline-kubernetes-events/#continue","title":"Continue","text":"<ul> <li>Continue to Configure Dynatrace OpenPipeline for Collector Logs</li> </ul>"},{"location":"9-dynatrace-openpipeline-collector-logs/","title":"Dynatrace OpenPipeline - Collector Logs","text":"<p>In this module we'll utilize Dynatrace OpenPipeline to process OpenTelemetry Collector logs at ingest, in order to make them easier to analyze and leverage.  The logs will be ingested by OpenTelemetry Collector, deployed on Kubernetes as part of the previous module.  The OpenTelemetry Collector logs are output mixed JSON/console format, making them difficult to use by default.  With OpenPipeline, the logs will be processed at ingest, to manipulate fields, extract metrics, and raise alert events in case of any issues.</p> <p>Goals:</p> <ul> <li>Parse JSON content</li> <li>Set loglevel and status</li> <li>Remove unwanted fields/attributes</li> <li>Extract metrics: successful data points</li> <li>Extract metrics: dropped data points</li> <li>Alert: zero data points</li> <li>Storage retention with bucket assignment</li> </ul> <p></p> <p>OpenPipeline is an architectural component of Dynatrace SaaS.  It resides between the Dynatrace SaaS tenant and Grail data lakehouse.  Logs (,traces, metrics, events, and more) are sent to the Dynatrace SaaS tenant and route through OpenPipeline where they are enriched, transformed, and contextualized prior to being stored in Grail.</p> <ul> <li>Learn More</li> </ul>"},{"location":"9-dynatrace-openpipeline-collector-logs/#query-logs","title":"Query Logs","text":"<p>Query and discover the OpenTelemetry Collector logs as they are ingested and stored in Dynatrace.  Use Dynatrace Query Language (DQL) to transform the logs at query time and prepare for Dynatrace OpenPipeline configuration.</p> <p>Import Notebook into Dynatrace</p> <p>Download OpenTelemetry Collector Logs Notebook</p> <p>OpenTelemetry Collector Logs - Ondemand Processing at Query Time (Notebook)</p> <p>The OpenTelemetry Collector can be configured to output JSON structured logs as internal telemetry.  Dynatrace DQL can be used to filter, process, and analyze this log data to ensure reliability of the OpenTelemetry data pipeline.</p> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| sort timestamp desc\n| limit 50\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#parse-json-content","title":"Parse JSON Content","text":"<p>Parse Command Documentation</p> <p>Parses a record field and puts the result(s) into one or more fields as specified in the pattern.  The parse command works in combination with the Dynatrace Pattern Language for parsing strings.</p> <p>Parse JSON Object Documentation</p> <p>There are several ways how to control parsing elements from a JSON object. The easiest is to use the JSON matcher without any parameters. It will enumerate all elements, transform them into Log processing data type from their defined type in JSON and returns a variant_object with parsed elements.</p> <p>The <code>content</code> field contains JSON structured details that can be parsed to better analyze relevant fields. The structured content can then be flattened for easier analysis.</p> <p>FieldsFlatten Command Documentation</p> <p>Sample: <pre><code>{\n  \"level\": \"info\",\n  \"ts\": \"2025-12-31T19:36:45.773Z\",\n  \"msg\": \"Logs\",\n  \"otelcol.component.id\": \"debug\",\n  \"otelcol.component.kind\": \"Exporter\",\n  \"otelcol.signal\": \"logs\",\n  \"resource logs\": \"131\",\n  \"log records\": \"800\"\n}\n</code></pre></p> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| sort timestamp desc\n| limit 50\n| parse content, \"JSON:jc\"\n| fieldsFlatten jc, prefix: \"content.\"\n| fieldsKeep timestamp, app.label.name, content, jc, \"content.*\"\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#set-loglevel-and-status-fields","title":"Set <code>loglevel</code> and <code>status</code> fields","text":"<p>Selection and Modification Documentation</p> <p>The <code>fieldsAdd</code> command evaluates an expression and appends or replaces a field.</p> <p>The JSON structure contains a field <code>level</code> that can be used to set the <code>loglevel</code> field.  It must be uppercase.</p> <ul> <li>loglevel possible values are: NONE, TRACE, DEBUG, NOTICE, INFO, WARN, SEVERE, ERROR, CRITICAL, ALERT, FATAL, EMERGENCY</li> <li>status field possible values are: ERROR, WARN, INFO, NONE</li> </ul> <p>The <code>if</code> conditional function allows you to set a value based on a conditional expression.  Since the <code>status</code> field depends on the <code>loglevel</code> field, a nested <code>if</code> expression can be used.</p> <p>If Function Documentation</p> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| sort timestamp desc\n| limit 50\n| parse content, \"JSON:jc\"\n| fieldsFlatten jc, prefix: \"content.\"\n| fieldsAdd loglevel = upper(content.level)\n| fieldsAdd status = if(loglevel==\"INFO\",\"INFO\",else: // most likely first\n                     if(loglevel==\"WARN\",\"WARN\",else: // second most likely second\n                     if(loglevel==\"ERROR\",\"ERROR\", else: // third most likely third\n                     if(loglevel==\"NONE\",\"NONE\",else: // fourth most likely fourth\n                     if(loglevel==\"TRACE\",\"INFO\",else:\n                     if(loglevel==\"DEBUG\",\"INFO\",else:\n                     if(loglevel==\"NOTICE\",\"INFO\",else:\n                     if(loglevel==\"SEVERE\",\"ERROR\",else:\n                     if(loglevel==\"CRITICAL\",\"ERROR\",else:\n                     if(loglevel==\"ALERT\",\"ERROR\",else:\n                     if(loglevel==\"FATAL\",\"ERROR\",else:\n                     if(loglevel==\"EMERGENCY\",\"ERROR\",else:\n                     \"NONE\"))))))))))))\n| fields timestamp, loglevel, status, content, content.level\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#remove-unwanted-fieldsattributes","title":"Remove unwanted fields/attributes","text":"<p>The <code>fieldsRemove</code> command will remove selected fields.</p> <p>FieldsRemove Command Documentation</p> <p>After parsing and flattening the JSON structured content, the original fields should be removed.  Fields that don't add value should be removed at the source, but if they are not, they can be removed with DQL.</p> <p>Every log record should ideally have a content field, as it is expected.  The <code>content</code> field can be updated with values from other fields, such as <code>content.msg</code> and <code>content.message</code>.</p> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| sort timestamp desc\n| limit 50\n| parse content, \"JSON:jc\"\n| fieldsFlatten jc, prefix: \"content.\"\n| fieldsRemove jc, content.level, content.ts, log.iostream\n| fieldsAdd content = if((isNotNull(content.msg) and isNotNull(content.message)), concat(content.msg,\" | \",content.message), else:\n                      if((isNotNull(content.msg) and isNull(content.message)), content.msg, else:\n                      if((isNull(content.msg) and isNotNull(content.message)), content.message, else:\n                      content)))\n| fields timestamp, content, content.msg\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#extract-successful-data-points-metric","title":"Extract Successful Data Points Metric","text":"<p>The <code>summarize</code> command enables you to aggregate records to compute results based on counts, attribute values, and more.</p> <p>Summarize Command Documentation</p> <p>The JSON structured content contains several fields that indicate the number of successful data points / signals sent by the exporter.</p> <ul> <li>logs: resource logs, log records</li> <li>metrics: resource metrics, metrics, data points</li> <li>traces: resource spans, spans</li> </ul> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| sort timestamp desc\n| parse content, \"JSON:jc\"\n| fieldsFlatten jc, prefix: \"content.\"\n| filter matchesValue(`content.otelcol.component.kind`,\"Exporter\")\n| fieldsRemove content, jc, content.level, content.ts, log.iostream\n| summarize {\n              resource_metrics = sum(`content.resource metrics`),\n              metrics = sum(`content.metrics`),\n              data_points = sum(`content.data points`),\n              resource_spans = sum(`content.resource spans`),\n              spans = sum(`content.spans`),\n              resource_logs = sum(`content.resource logs`),\n              log_records = sum(`content.log records`)\n              }, by: { signal = `content.otelcol.signal`, exporter = `content.otelcol.component.id`, collector = app.label.name, k8s.cluster.name}\n</code></pre></p> <p>Result:</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#extract-dropped-data-points-metric","title":"Extract Dropped Data Points Metric","text":"<p>The JSON structured content contains several fields that indicate the number of dropped data points / signals sent by the exporter.</p> <ul> <li>dropped items</li> <li>signal</li> <li>(exporter) name</li> <li>collector</li> </ul> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| sort timestamp desc\n| parse content, \"JSON:jc\"\n| fieldsFlatten jc, prefix: \"content.\"\n| filter matchesValue(`content.otelcol.component.kind`,\"exporter\")\n| filter matchesValue(`content.level`,\"error\") and isNotNull(`content.dropped_items`)\n| summarize dropped_items = sum(`content.dropped_items`), by: {signal = `content.otelcol.signal`, collector = app.label.name, component = `content.otelcol.component.id`}\n</code></pre></p> <p>Result:</p> <p></p> <p>You likely won't have any data matching your query as you shouldn't have data drops.  You can force data drops by toggling your Dynatrace API Access Token off for a couple minutes and then turning it back on.</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#alert-on-zero-data-points","title":"Alert on Zero Data Points","text":"<p>It would be unexpected that the collector exporter doesn't send any data points or signals.  We could alert on this unexpected behavior.</p> <p>The field <code>content.otelcol.signal</code> will indicate the type of data point or signal.  The fields <code>content.log records</code>, <code>content.data points</code>, and <code>content.spans</code> will indicate the number of signals sent.  If the value is <code>0</code>, that is unexpected.</p> <p>Query logs in Dynatrace</p> <p>DQL: <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| sort timestamp desc\n| limit 100\n| parse content, \"JSON:jc\"\n| fieldsFlatten jc, prefix: \"content.\"\n| filter matchesValue(`content.otelcol.component.kind`,\"exporter\")\n| summarize {\n              logs = countIf(matchesValue(`content.otelcol.signal`,\"logs\") and matchesValue(toString(`content.log records`),\"0\")),\n              metrics = countIf(matchesValue(`content.otelcol.signal`,\"metrics\") and matchesValue(toString(`content.data points`),\"0\")),\n              traces = countIf(matchesValue(`content.otelcol.signal`,\"traces\") and matchesValue(toString(`content.spans`),\"0\"))\n            }, by: {signal = `content.otelcol.signal`, collector = app.label.name}\n</code></pre></p> <p>Result:</p> <p></p> <p>DQL in Notebooks Summary</p> <p>DQL gives you the power to filter, parse, summarize, and analyze log data quickly and on the fly.  This is great for use cases where the format of your log data is unexpected.  However, when you know the format of your log data and you know how you will want to use that log data in the future, you'll want that data to be parsed and presented a certain way during ingest.  OpenPipeline provides the capabilites needed to accomplish this.</p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#configure-openpipeline","title":"Configure OpenPipeline","text":"<p>Configure Dynatrace OpenPipeline for OpenTelemetry Collector logs.</p> <p>View Images</p> <p>If the images are too small and the text is difficult to read, right-click and open the image in a new tab.</p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p> <p>In your Dynatrace tenant, launch the (new) <code>Settings</code> app.  From the <code>Process and contextualize</code> menu, click on <code>OpenPipeline</code>.</p> <p></p> <p>OpenPipeline App</p> <p>Depending on your Dynatrace tenant version, you may need to use the OpenPipeline app instead.</p> <p>Begin by selecting <code>Logs</code> from the menu of telemetry types.  Then choose <code>Pipelines</code>.  Click on <code>+ Pipeline</code> to add a new pipeline.</p> <p></p> <p>Name the new pipeline, <code>OpenTelemetry Collector Logs</code>.  Click on the <code>Processing</code> tab to begin adding <code>Processor</code> rules.</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#parse-json-content_1","title":"Parse JSON Content","text":"<p>Add a processor to parse the JSON structured content field.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Parse JSON Content\n</code></pre></p> <p>Matching condition: <pre><code>k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n</code></pre></p> <p>Processor definition: <pre><code>parse content, \"JSON:jc\"\n| fieldsFlatten jc, prefix: \"content.\"\n</code></pre></p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#loglevel-and-status","title":"Loglevel and Status","text":"<p>Add a processor to set the loglevel and status fields.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Set loglevel and status fields\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(`content.level`)\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd loglevel = upper(content.level)\n| fieldsAdd status = if(loglevel==\"INFO\",\"INFO\",else: // most likely first\n                     if(loglevel==\"WARN\",\"WARN\",else: // second most likely second\n                     if(loglevel==\"ERROR\",\"ERROR\", else: // third most likely third\n                     if(loglevel==\"NONE\",\"NONE\",else: // fourth most likely fourth\n                     if(loglevel==\"TRACE\",\"INFO\",else:\n                     if(loglevel==\"DEBUG\",\"INFO\",else:\n                     if(loglevel==\"NOTICE\",\"INFO\",else:\n                     if(loglevel==\"SEVERE\",\"ERROR\",else:\n                     if(loglevel==\"CRITICAL\",\"ERROR\",else:\n                     if(loglevel==\"ALERT\",\"ERROR\",else:\n                     if(loglevel==\"FATAL\",\"ERROR\",else:\n                     if(loglevel==\"EMERGENCY\",\"ERROR\",else:\n                     \"NONE\"))))))))))))\n</code></pre></p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#remove-fields","title":"Remove Fields","text":"<p>Add a processor to remove the extra and unwanted fields.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Remove unwanted fields/attributes\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(jc) and isNotNull(loglevel) and isNotNull(status) and loglevel!=\"NONE\"\n</code></pre></p> <p>DQL processor definition <pre><code>fieldsRemove jc, content.level, content.ts, log.iostream\n| fieldsAdd content = if((isNotNull(content.msg) and isNotNull(content.message)), concat(content.msg,\" | \",content.message), else:\n                      if((isNotNull(content.msg) and isNull(content.message)), content.msg, else:\n                      if((isNull(content.msg) and isNotNull(content.message)), content.message, else:\n                      content)))\n</code></pre></p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#remove-spaces-from-metrics-fields","title":"Remove Spaces from Metrics Fields","text":"<p>Add a processor to remove the spaces from the metrics fields.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Metric extraction - remove spaces from fields - metrics\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.component.kind`,\"exporter\") and matchesValue(`content.otelcol.signal`,\"metrics\")\n</code></pre></p> <p>DQL processor definition <pre><code>fieldsAdd content.resource_metrics = `content.resource metrics`\n| fieldsAdd content.data_points = `content.data points`\n| fieldsRemove `content.resource metrics`, `content.data points`\n</code></pre></p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#remove-spaces-from-logs-fields","title":"Remove Spaces from Logs Fields","text":"<p>Add a processor to remove the spaces from the logs fields.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Metric extraction - remove spaces from fields - logs\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.component.kind`,\"exporter\") and matchesValue(`content.otelcol.signal`,\"logs\")\n</code></pre></p> <p>DQL processor definition <pre><code>fieldsAdd content.resource_logs = `content.resource logs`\n| fieldsAdd content.log_records = `content.log records`\n| fieldsRemove `content.resource logs`, `content.log records`\n</code></pre></p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#remove-spaces-from-traces-fields","title":"Remove Spaces from Traces Fields","text":"<p>Add a processor to remove the spaces from the traces fields.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Metric extraction - remove spaces from fields - traces\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.component.kind`,\"exporter\") and matchesValue(`content.otelcol.signal`,\"traces\")\n</code></pre></p> <p>DQL processor definition <pre><code>fieldsAdd content.resource_spans = `content.resource spans`\n| fieldsRemove `content.resource spans`\n</code></pre></p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#collector-attribute","title":"Collector Attribute","text":"<p>Add a processor to add the collector attribute from the app.label.name field.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Add collector attribute from app.label.name\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(app.label.name)\n</code></pre></p> <p>DQL processor definition <pre><code>fieldsAdd collector = app.label.name\n</code></pre></p> <p></p> <p>Consider Saving</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#zero-data-points-for-metrics","title":"Zero Data Points for Metrics","text":"<p>Switch to the <code>Data extraction</code> tab.</p> <p>Add a processor to extract a <code>Davis Event</code>.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Zero data points / signals - metrics\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.signal`,\"metrics\") and `content.data_points` == 0\n</code></pre></p> <p>Event Name: <pre><code>OpenTelemetry Collector - Zero Data Points - Metrics\n</code></pre></p> <p>Event description: <pre><code>The OpenTelemetry Collector has sent zero data points for metrics.\n</code></pre></p> <p>Additional event properties:</p> Property Value collector {collector} k8s.cluster.name {k8s.cluster.name} k8s.pod.name {k8s.pod.name} <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#zero-data-points-for-logs","title":"Zero Data Points for Logs","text":"<p>Add a processor to extract a <code>Davis Event</code>.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Zero data points / signals - logs\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.signal`,\"logs\") and `content.log_records` == 0\n</code></pre></p> <p>Event Name: <pre><code>OpenTelemetry Collector - Zero Data Points - Logs\n</code></pre></p> <p>Event description: <pre><code>The OpenTelemetry Collector has sent zero data points for logs.\n</code></pre></p> <p>Additional event properties:</p> Property Value collector {collector} k8s.cluster.name {k8s.cluster.name} k8s.pod.name {k8s.pod.name} <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#zero-data-points-for-traces","title":"Zero Data Points for Traces","text":"<p>Add a processor to extract a <code>Davis Event</code>.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Zero data points / signals - traces\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.signal`,\"traces\") and `content.spans` == 0\n</code></pre></p> <p>Event Name: <pre><code>OpenTelemetry Collector - Zero Data Points - Traces\n</code></pre></p> <p>Event description: <pre><code>The OpenTelemetry Collector has sent zero data points for traces.\n</code></pre></p> <p>Additional event properties:</p> Property Value collector {collector} k8s.cluster.name {k8s.cluster.name} k8s.pod.name {k8s.pod.name} <p></p> <p>Consider Saving</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#successful-data-points-for-metrics","title":"Successful Data Points for Metrics","text":"<p>Switch to the <code>Metric Extraction</code> tab.</p> <p>Add a processor to extract a metric for successful metric data points from the exporter logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Successful data points - metrics\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.component.kind`,\"exporter\") and matchesValue(`content.otelcol.signal`,\"metrics\")\n</code></pre></p> <p>Field Extraction: <pre><code>content.data_points\n</code></pre></p> <p>Metric Key: <pre><code>otelcol_exporter_sent_metric_data_points\n</code></pre></p> <p>Dimensions:</p> Dimension collector k8s.cluster.name k8s.pod.name <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#successful-data-points-for-logs","title":"Successful Data Points for Logs","text":"<p>Add a processor to extract a metric for successful log data points from the exporter logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Successful data points - logs\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.component.kind`,\"exporter\") and matchesValue(`content.otelcol.signal`,\"logs\")\n</code></pre></p> <p>Field Extraction: <pre><code>content.log_records\n</code></pre></p> <p>Metric Key: <pre><code>otelcol_exporter_sent_log_records\n</code></pre></p> <p>Dimensions:</p> Dimension collector k8s.cluster.name k8s.pod.name <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#successful-data-points-for-traces","title":"Successful Data Points for Traces","text":"<p>Add a processor to extract a metric for successful trace data points from the exporter logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Successful data points - traces\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.component.kind`,\"exporter\") and matchesValue(`content.otelcol.signal`,\"traces\")\n</code></pre></p> <p>Field Extraction: <pre><code>content.spans\n</code></pre></p> <p>Metric Key: <pre><code>otelcol_exporter_sent_trace_spans\n</code></pre></p> <p>Dimensions:</p> Dimension collector k8s.cluster.name k8s.pod.name <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#dropped-data-points","title":"Dropped Data Points","text":"<p>Add a processor to extract a metric for dropped data points from the exporter logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Name: <pre><code>Dropped data points\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(`content.otelcol.component.kind`,\"exporter\") and isNotNull(`content.dropped_items`) and isNotNull(`content.otelcol.signal`)\n</code></pre></p> <p>Field Extraction: <pre><code>content.dropped_data_points\n</code></pre></p> <p>Metric Key: <pre><code>otelcol_exporter_dropped_data_points_by_data_type\n</code></pre></p> <p>Dimensions:</p> Dimension collector k8s.cluster.name k8s.pod.name <p></p> <p>Consider Saving</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#storage","title":"Storage","text":"<p>Switch to the <code>Storage</code> tab.</p> <p>Add a processor to set the bucket assignment.  Click on <code>+ Processor</code> to add a new  Bucket Assignment processor.</p> <p>Name: <pre><code>Infrastructure Observability and AIOps Bucket\n</code></pre></p> <p>Matching condition: <pre><code>true\n</code></pre></p> <p>Storage: <pre><code>Infrastructure Observability and AIOps (365 Days)\n</code></pre></p> <p></p> <p>This will result in all OpenTelemetry Collector logs matching this pipeline to be stored for 365 days in this bucket.</p> <p>The pipeline is now configured, click on <code>Save</code> to save the pipeline configuration.</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#dynamic-route","title":"Dynamic Route","text":"<p>A pipeline will not have any effect unless logs are configured to be routed to the pipeline.  With dynamic routing, data is routed based on a matching condition. The matching condition is a DQL query that defines the data set you want to route.</p> <p>Click on <code>Dynamic Routing</code> to configure a route to the target pipeline.  Click on <code>+ Dynamic Route</code> to add a new route.</p> <p></p> <p>Configure the <code>Dynamic Route</code> to use the <code>OpenTelemetry Collector Logs</code> pipeline.</p> <p>Name: <pre><code>OpenTelemetry Collector Logs\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.namespace.name,\"dynatrace\") and matchesValue(k8s.container.name,\"otc-container\") and matchesValue(telemetry.sdk.name,\"opentelemetry\")\n</code></pre></p> <p>Pipeline: <pre><code>OpenTelemetry Collector Logs\n</code></pre></p> <p>Click <code>Add</code> to add the route.</p> <p></p> <p>Validate that the route is enabled in the <code>Status</code> column.  Click on <code>Save</code> to save the dynamic route table configuration.</p> <p></p> <p>Allow <code>dynatrace</code> OpenTelemetry Collectors to generate new log data that will be routed through the new pipeline (3-5 minutes).</p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#analyze-results","title":"Analyze Results","text":"<p>Analyze the OpenTelemetry Collector logs after Dynatrace OpenPipeline processing.  Run the queries from the Notebook.</p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#analyze-the-results-in-dynatrace-notebook","title":"Analyze the results in Dynatrace (Notebook)","text":"<p>Query the OpenTelemetry Collector logs that have been processed by Dynatrace OpenPipeline.</p> <p>DQL: OpenPipeline Processing Results <pre><code>fetch logs\n| filter k8s.namespace.name == \"dynatrace\" and k8s.container.name == \"otc-container\" and telemetry.sdk.name == \"opentelemetry\"\n| fieldsRemove cloud.account.id // removed for data privacy and security reasons only\n| sort timestamp desc\n| limit 50\n| fieldsKeep timestamp, collector, k8s.cluster.name, loglevel, status, \"content.*\", content\n</code></pre></p> <p>Result:</p> <p></p> <p>The logs are now parsed at ingest into a format that simplifies our queries and makes them easier to use, especially for users that don't work with these log sources or Dynatrace DQL on a regular basis.</p> <p>Query the new log metric extracted by Dynatrace OpenPipeline, using the <code>timeseries</code> command.</p> <p>DQL: Extracted metrics: successful data points / signals <pre><code>timeseries { logs = sum(log.otelcol_exporter_sent_log_records) }, by: {k8s.cluster.name, collector}\n</code></pre></p> <p>Result:</p> <p></p> <p>By extracting the metric(s) at ingest time, the data points are stored long term and can easily be used in dashboards, anomaly detection, and automations.</p> <p>Metric Extraction Documentation</p> <p>Query the new dropped data points / signals metric extracted by Dynatrace OpenPipeline, using the <code>timeseries</code> command.</p> <p>DQL: Extracted metrics: dropped data points / signals <pre><code>timeseries { dropped_items = sum(log.otelcol_exporter_dropped_items_by_signal, default: 0) }, by: {k8s.cluster.name, collector, signal}\n</code></pre></p> <p>Result:</p> <p></p> <p>You likely won't have any data matching your query as you shouldn't have data drops.  You can force data drops by toggling your Dynatrace API Access Token off for a couple minutes and then turning it back on.</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#opentelemetry-collector-dashboard","title":"OpenTelemetry Collector Dashboard","text":"<p>Import Dashboard into Dynatrace</p> <p>OpenTelemetry Collector Dashboard</p> <p>Explore the OpenTelemetry Collector [IsItObservable] - OpenPipeline Dashboard that you imported.</p> <p></p>"},{"location":"9-dynatrace-openpipeline-collector-logs/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully set up Dynatrace OpenPipeline pipelines to process the OpenTelemetry logs at ingest.</p> <ul> <li>OpenTelemetry Collector logs<ul> <li>Parse JSON structured content field to easily filter, aggregate, and analyze on nested fields</li> <li>Set loglevel and status fields to easily identify errors with the OpenTelemetry Collector</li> <li>Remove unwanted fields/attributes to reduce log bloat and optimize queries</li> <li>Extract metrics: successful data points to track OpenTelemetry Collector health and reduce heavy log queries</li> <li>Extract metrics: dropped data points to track OpenTelemetry Collector health and reduce heavy log queries</li> <li>Alert: zero data points to be alerted on OpenTelemetry Collector health issues</li> <li>Routed logs to a specific bucket in Grail based on retention period needs</li> </ul> </li> </ul>"},{"location":"9-dynatrace-openpipeline-collector-logs/#continue","title":"Continue","text":"<ul> <li>Continue to Cleanup</li> </ul>"},{"location":"cleanup/","title":"10. Cleanup","text":"<p>Deleting the codespace from inside the container</p> <p>We like to make your life easier, for convenience there is a function loaded in the shell of the Codespace for deleting the codespace, just type <code>deleteCodespace</code>. This will trigger the deletion of the codespace.</p> <p>Another way to do this is by going to https://github.com/codespaces and delete the codespace.</p> <p>You may also want to deactivate or delete the API token needed for this lab.</p>"},{"location":"snippets/admonitions/","title":"Admonitions","text":"<p>Warning</p> <p>This is a Warning </p> <p>Note</p> <p>This is a Note </p> <p>Important</p> <p>This is important </p> <p>Tipp</p> <p>This is a tipp </p>"},{"location":"snippets/disclaimer/","title":"Disclaimer","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"snippets/grail-requirements/","title":"Grail requirements","text":"<p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"snippets/view-code/","title":"View code","text":"<p>View the Code</p> <p>The code for this repository is hosted on GitHub. Click the \"View Code on GitHub\" link above.</p>"}]}