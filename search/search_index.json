{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"1. About","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"#lab-overview","title":"Lab Overview","text":"<p>During this hands-on training, we\u2019ll learn how to capture logs from Kubernetes using OpenTelemetry and ship them to Dynatrace for analysis.  This will demonstrate how to use Dynatrace with OpenTelemetry; without any Dynatrace native components installed on the Kubernetes cluster (Operator, OneAgent, ActiveGate, etc.).  We'll then utilize Dynatrace OpenPipeline to process OpenTelemetry logs at ingest, to manipulate fields, extract metrics, raise alert events, and manage retention periods, in order to make them easier to analyze and leverage.</p> <p>Lab tasks:</p> <ol> <li>Deploy OpenTelemetry Collector for logs, traces, and metrics</li> <li>Create custom Buckets for Grail storage management</li> <li>Process Astronomy Shop logs with Dynatrace OpenPipeline</li> <li>Query and visualize logs and metrics in Dynatrace using DQL</li> </ol>"},{"location":"#technical-specification","title":"Technical Specification","text":""},{"location":"#technologies-used","title":"Technologies Used","text":"<ul> <li>Dynatrace</li> <li>Kubernetes Kind<ul> <li>tested on Kind tag 0.27.0</li> </ul> </li> <li>Cert Manager - *prerequisite for OpenTelemetry Operator<ul> <li>tested on cert-manager v1.14.4</li> </ul> </li> <li>OpenTelemetry Operator<ul> <li>tested on v0.103.0 (June 2024)</li> </ul> </li> <li>OpenTelemetry Collector - Dynatrace Distro<ul> <li>tested on v0.25.0 (March 2025)</li> </ul> </li> <li>OpenTelemetry Collector - Contrib Distro<ul> <li>tested on v0.121.0 (March 2025)</li> </ul> </li> <li>OpenTelemetry AstronomyShop Helm Chart<ul> <li>tested on v0.31.0 (June 2024)</li> </ul> </li> </ul>"},{"location":"#reference-architecture","title":"Reference Architecture","text":"<p>OpenTelemetry Astronomy Shop Demo Architecture</p>"},{"location":"#continue","title":"Continue","text":"<ul> <li>Continue to Getting Started (Prerequisites)</li> </ul>"},{"location":"2-getting-started/","title":"2. Getting started","text":"<p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"2-getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Identify Dynatrace OTLP Endpoint</li> <li>Generate Dynatrace Access Token</li> <li>Identify Your Initials</li> <li>Add Bucket Storage Management Permissions</li> </ul>"},{"location":"2-getting-started/#identify-dynatrace-otlp-endpoint","title":"Identify Dynatrace OTLP Endpoint","text":"<p>The OpenTelemetry Protocol (OTLP) is the principal network protocol for the exchange of telemetry data between OpenTelemetry-backed services and applications.  The Dynatrace SaaS tenant provides an OTLP endpoint.</p> <p>See Related Export with OTLP Documentation</p> <p>Identify and save/store your OTLP endpoint for the Dynatrace SaaS tenant:</p> <p>No Trailing Slash</p> <p>Do not include a trailing slash!</p> Type URL Pattern Live (Prod) https://{your-environment-id}.live.dynatrace.com/api/v2/otlp Stage https://{your-environment-id}.sprint.dynatracelabs.com/api/v2/otlp ActiveGate https://{your-activegate-domain}:9999/e/{your-environment-id}/api/v2/otlp"},{"location":"2-getting-started/#generate-dynatrace-access-token","title":"Generate Dynatrace Access Token","text":"<p>Generate a new API access token with the following scopes: <pre><code>Ingest events\nIngest logs\nIngest metrics\nIngest OpenTelemetry traces\n</code></pre> See Related Dynatrace API Token Creation Documentation</p> <p></p>"},{"location":"2-getting-started/#identify-your-initials","title":"Identify Your Initials","text":"<p>In this lab, we'll uniquely identify your OpenTelemetry data using your initials; in case you are using a shared tenant.  We'll be using <code>&lt;INITIALS&gt;-k8s-otel-o11y</code> as our pattern.  Identify your initials (3-5 characters) and use them whenever prompted during the lab.</p>"},{"location":"2-getting-started/#add-storage-management-permissions","title":"Add Storage Management Permissions","text":"<p>The Grail data model consists of buckets, tables, and views.  Records are stored in buckets.  Buckets are assigned to tables, including logs, metrics, events, security events, and bizevents tables. Fetching from a table returns all records from all buckets that are assigned to that table.  To manage your buckets, ensure that you have configured the following permissions:</p> <ul> <li>storage:bucket-definitions:read</li> <li>storage:bucket-definitions:write</li> <li>storage:bucket-definitions:delete</li> <li>storage:bucket-definitions:truncate</li> </ul> <p>Policy definition:</p> <pre><code>ALLOW storage:bucket-definitions:write;\nALLOW storage:bucket-definitions:read;\nALLOW storage:bucket-definitions:delete;\nALLOW storage:bucket-definitions:truncate;\n</code></pre> <p></p> <p>After creating the policy, be sure to add/bind it to a group that you belong to.</p>"},{"location":"2-getting-started/#continue","title":"Continue","text":"<ul> <li>Continue to Codespaces Setup</li> </ul>"},{"location":"3-codespaces/","title":"3. Codespaces","text":""},{"location":"3-codespaces/#create-codespace","title":"Create Codespace","text":"<p>Click to open Codespaces for this lab repository:</p> <p></p> <p>Codespace Configuration</p> <ul> <li>Branch<ul> <li>select the main branch</li> </ul> </li> <li>Dev container configuration<ul> <li>select Enablement on codespaces template</li> </ul> </li> <li>Machine type<ul> <li>select 4-core</li> </ul> </li> <li>Region<ul> <li>select any region, preferably one closest to your Dynatrace tenant</li> </ul> </li> </ul>"},{"location":"3-codespaces/#wait-for-codespace","title":"Wait for Codespace","text":"<p>We know your time is very valuable. This codespace takes around 7-10 minutes to be fully operational. A local Kubernetes (kind) cluster monitored by Dynatrace will be configured and in it a sample application, Astronomy Shop, will be deployed. To make your experience better, we are also installing and configuring tools like:</p> <p>k9s kubectl helm node jq python3 gh</p>"},{"location":"3-codespaces/#explore-codespace-optional","title":"Explore Codespace (optional)","text":"<p>Your Codespace has now deployed the following resources:</p> <ul> <li> <p>A local Kubernetes (kind) cluster monitored by Dynatrace, with some pre-deployed apps that will be used later in the demo.</p> </li> <li> <p>After a couple of minutes, you'll see this screen in your codespaces terminal. It contains the links to the locally exposed labguide and the UI of the application which we will be doing our hands-on training with.</p> </li> </ul> <p>Sample output: </p>"},{"location":"3-codespaces/#tips-tricks","title":"Tips &amp; Tricks","text":"<p>We want to boost your learning and try to make your experience as smooth as possible with Dynatrace trainings. Your Codespaces have a couple of convenience features added. </p>"},{"location":"3-codespaces/#show-the-greeting","title":"Show the greeting","text":"<p>In the terminal, there are functions loaded for your convenience. By creating a new Terminal the Greeting will be shown that includes the links to the exposed apps, the Github  pages, the Github Repository, the Dynatrace Tenant that is bound to this devcontainer and some of the tools installed.</p> <p>You can create a new Terminal directly in VSCode, type <code>zsh</code> or call the function <code>printGreeting</code> and that will print the greeting with the most relevant information.</p>"},{"location":"3-codespaces/#navigating-in-your-local-kubernetes","title":"Navigating in your local Kubernetes","text":"<p>The client <code>kubectl</code> and <code>k9s</code>are configured so you can navigate in your local Kubernetes like butter.  </p>"},{"location":"3-codespaces/#exposing-the-apps-to-the-public","title":"Exposing the apps to the public","text":"<p>The apps MKdocs and Astronomy Shop are being exposed in the devcontainer to your localhost. If you want to make the endpoints public accesible, just go to the ports section in VsCode, right click on them and change the visibility to public.</p>"},{"location":"3-codespaces/#troubleshooting","title":"Troubleshooting","text":""},{"location":"3-codespaces/#astronomy-shop","title":"Astronomy Shop","text":"<p>If you encounter problems with the Astronomy Shop app deployed in the <code>astronomy-shop</code> namespace, recycle the pods and expose the app.</p> <p>Recycle pods: <pre><code>kubectl delete pods -n astronomy-shop --field-selector=\"status.phase=Running\"\n</code></pre></p> <p>Expose app: <pre><code>exposeAstronomyShop\n</code></pre></p>"},{"location":"3-codespaces/#opentelemetry-capstone","title":"OpenTelemetry Capstone","text":"<p>Capstone Module</p> <p>What is a Capstone? A hands-on, culminating learning experience where participants apply the knowledge and skills they've gained throughout a course or program to solve a real-world problem, create a project, or present a comprehensive solution.</p> <p>In this module we'll utilize multiple OpenTelemetry Collectors to collect application traces/spans, log records, and metric data points generated by OpenTelemetry, from a Kubernetes cluster and ship them to Dynatrace.  This is a capstone lab that utilizes the concepts of the previous Kubernetes OpenTelemetry labs.</p> <p>Goals:</p> <ol> <li>Deploy 4 OpenTelemetry Collectors</li> <li>Configure OpenTelemetry Collector service pipeline for data enrichment</li> <li>Analyze metrics, traces, and logs in Dynatrace</li> </ol> <p>Define workshop user variables</p> <p>Sprint Environment</p> <p>Are you using a Sprint environment for your Dynatrace tenant?  If so, then use <code>export DT_ENDPOINT=https://{your-environment-id}.sprint.dynatracelabs.com/api/v2/otlp</code> instead of the <code>live</code> version below.</p> <p>In your Github Codespaces Terminal: <pre><code>export DT_ENDPOINT=https://{your-environment-id}.live.dynatrace.com/api/v2/otlp\nexport DT_API_TOKEN={your-api-token}\nexport NAME=&lt;INITIALS&gt;-k8s-otel-o11y\n</code></pre></p> <p>Move into the lab module directory</p> <p>Command:</p> <pre><code>cd $BASE_DIR/lab-modules/opentelemetry-capstone\n</code></pre> <p>Deploy OpenTelemetry Capstone</p> <p>Command:</p> <pre><code>deployOpenTelemetryCapstone\n</code></pre> <p>Validate the OpenTelemetry data using the Astronomy Shop Dashboard.</p> <p></p> <p>Download astronomy-shop Dashboard</p>"},{"location":"3-codespaces/#continue","title":"Continue","text":"<ul> <li>Continue to Dynatrace Buckets</li> </ul>"},{"location":"4-dynatrace-buckets/","title":"Dynatrace Buckets (Storage Management)","text":"<p>In this module we'll create Grail storage Buckets to manage log data.  By default, all logs are stored in a default logs bucket in Grail - that retains log data for 35 days.  We can create custom buckets with different retention periods to solve for our log management and analytics use cases.</p> <p>You can create a bucket tailored to your needs. Grail buckets behave like folders in a file system and are designed for records that should be handled together. For example, you might need to store together:</p> <ul> <li>Data with the same retention period</li> <li>Data that needs to be queried/analyzed together</li> <li>Data that needs to be deleted at the same time</li> </ul> <p>Defining buckets can improve query performance by reducing query execution time and the scope of data read. Finally, having your data stored in a bucket streamlines your permission management because you can easily provide a user group or single users with access to needed data.</p> <ul> <li>Learn More</li> </ul> <p>Goals:</p> <ol> <li>View existing buckets within Grail storage management</li> <li>Create new custom buckets for log data</li> </ol>"},{"location":"4-dynatrace-buckets/#view-buckets","title":"View Buckets","text":"<p>In your Dynatrace tenant, use the Search function (<code>cmd + k</code> / <code>ctrl + k</code>) and search for <code>bucket</code>.  Click to open <code>Bucket storage management</code> under the Settings category.</p> <p></p> <p>This will open the <code>Settings</code> app and the Buckets list under the Storage management menu.  Here you can view all existing buckets, including the default buckets and custom buckets.  Each bucket has a retention period (in days) and belongs to a specific table, where the table is a specific data type.</p> <p></p> <p>Change the <code>Select bucket type</code> dropdown to filter on <code>logs</code> buckets only.  Locate the <code>default_logs</code> bucket and notice the retention period of 35 days.  By default, all logs are stored in this bucket and will be retained for 35 days.</p> <p></p>"},{"location":"4-dynatrace-buckets/#create-buckets","title":"Create Buckets","text":""},{"location":"4-dynatrace-buckets/#95-day-retention","title":"95 Day Retention","text":"<p>Click on the <code>+ Bucket</code> button to add a new bucket.  This bucket will store the application logs we want to retain for a longer duration, 95 days.</p> <p>Configure the new bucket as follows:</p> <p>Bucket name: <pre><code>observe_and_troubleshoot_apps_95_days\n</code></pre></p> <p>Display name: <pre><code>Observe and Troubleshoot Apps (95 Days)\n</code></pre></p> <p>Retention period (in days): <pre><code>95\n</code></pre></p> <p>Bucket table type: <pre><code>logs\n</code></pre></p> <p>Click on <code>Create</code> to add the new bucket.</p> <p></p> <p>Logs License Model</p> <p>If your DPS subscription includes both logs license models (Usage based and Retain with Included Queries) you will have the option to configure which license model will be used by logs queried from this bucket.  For this lab module, select Usage based. </p> <p>It will take a minute or two for the new bucket to be generated.  Refresh the page until the bucket Status is <code>Active</code>.</p> <p></p> <p>Creating a new bucket has no effect on data storage in Grail.  Logs will continue to be stored in the existing buckets, including the default bucket.  We will use Dynatrace OpenPipeline in upcoming lab modules to assign log data to the new bucket.</p>"},{"location":"4-dynatrace-buckets/#365-day-retention","title":"365 Day Retention","text":"<p>Click on the <code>+ Bucket</code> button to add a new bucket.  This bucket will store the infrastructure logs that we are required to keep (theoretically) for an entire year.</p> <p>Configure the new bucket as follows:</p> <p>Bucket name: <pre><code>infrastructure_obs_ai_ops_365_days\n</code></pre></p> <p>Display name: <pre><code>Infrastructure Observability and AIOps (365 Days)\n</code></pre></p> <p>Retention period (in days): <pre><code>365\n</code></pre></p> <p>Bucket table type: <pre><code>logs\n</code></pre></p> <p>Click on <code>Create</code> to add the new bucket.</p> <p></p> <p>It will take a minute or two for the new bucket to be generated.  Refresh the page until the bucket Status is <code>Active</code>.</p> <p></p> <p>Creating a new bucket has no effect on data storage in Grail.  Logs will continue to be stored in the existing buckets, including the default bucket.  We will use Dynatrace OpenPipeline in upcoming lab modules to assign log data to the new bucket.</p>"},{"location":"4-dynatrace-buckets/#7-day-retention","title":"7 Day Retention","text":"<p>Click on the <code>+ Bucket</code> button to add a new bucket.  This bucket will store the various logs that we don't need to keep very long and can purge after a week.</p> <p>Configure the new bucket as follows:</p> <p>Bucket name: <pre><code>log_management_analytics_7_days\n</code></pre></p> <p>Display name: <pre><code>Log Management and Analytics (7 Days)\n</code></pre></p> <p>Retention period (in days): <pre><code>7\n</code></pre></p> <p>Bucket table type: <pre><code>logs\n</code></pre></p> <p>Click on <code>Create</code> to add the new bucket.</p> <p></p> <p>It will take a minute or two for the new bucket to be generated.  Refresh the page until the bucket Status is <code>Active</code>.</p> <p></p> <p>Creating a new bucket has no effect on data storage in Grail.  Logs will continue to be stored in the existing buckets, including the default bucket.  We will use Dynatrace OpenPipeline in upcoming lab modules to assign log data to the new bucket.</p>"},{"location":"4-dynatrace-buckets/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully created custom buckets for Grail storage management.</p> <ul> <li>Custom Logs Buckets<ul> <li>Logs for the <code>Observe and Troubleshoot Apps</code> use case, retained for 95 Days</li> <li>Logs for the <code>Infrastructure Observability and AIOps</code> use case, retained for 365 Days</li> <li>Logs for the <code>Log Management and Analytics</code> use case, retained for 7 Days</li> </ul> </li> </ul> <p>These retention periods were chosen arbitrarily for this lab, your retention periods in the real-world will vary.</p>"},{"location":"4-dynatrace-buckets/#continue","title":"Continue","text":"<ul> <li>Continue to Configuring OpenPipeline for Astronomy Shop logs</li> </ul>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/","title":"Dynatrace OpenPipeline - Astronomy Shop Logs","text":"<p>In this module we'll utilize Dynatrace OpenPipeline to process <code>astronomy-shop</code> application logs at ingest, in order to make them easier to analyze and leverage.  The logs will be ingested by OpenTelemetry Collector, deployed on Kubernetes as part of the previous module.  With OpenPipeline, the logs will be processed at ingest, to manipulate fields, extract metrics, and raise alert events in case of any issues.</p> <p>The OpenTelemetry Collector deployed as a Daemonset is collecting Pod logs from the Node's filesystem and shipping them to Dynatrace.  The application Pods from the Astronomy Shop application have been instrumented with the OpenTelemetry SDK.  The OpenTelemetry SDK is configured to ship logs (,traces, and metrics) to Dynatrace via the OpenTelemetry Collector deployed as a Deployment (Gateway).  Due to the differences in how these logs are collected, they do not contain the same metadata.  While these logs contain a lot of useful information, they are missing valuable fields/attributes that will make them easier to analyze in context.  These logs can be enriched at ingest, using OpenPipeline.  Additionally, OpenPipeline allows us to process fields, extract new data types, manage permissions, and modify storage retention.</p> <p>Goals:</p> <ul> <li>Add OpenTelemetry service name and namespace</li> <li>Enrich SDK logs with additional Kubernetes metadata</li> <li>Apply Dynatrace technology bundle (Java)</li> <li>Extract data: Payment transaction business event</li> <li>Extract metrics: Payment transaction amount</li> <li>Storage retention with bucket assignment</li> </ul> <p></p> <p>OpenPipeline is an architectural component of Dynatrace SaaS.  It resides between the Dynatrace SaaS tenant and Grail data lakehouse.  Logs (,traces, metrics, events, and more) are sent to the Dynatrace SaaS tenant and route through OpenPipeline where they are enriched, transformed, and contextualized prior to being stored in Grail.</p> <ul> <li>Learn More</li> </ul>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#query-logs","title":"Query Logs","text":"<p>Query and discover the Astronomy Shop logs as they are ingested and stored in Dynatrace.  Use Dynatrace Query Language (DQL) to transform the logs at query time and prepare for Dynatrace OpenPipeline configuration.</p> <p>Import Notebook into Dynatrace</p> <p>Download Astronomy Shop Logs Notebook</p> <p>Astronomy Shop Logs - Ondemand Processing at Query Time (Notebook)</p> <p>In OpenTelemetry, <code>service.name</code> and <code>service.namespace</code> are used to provide meaningful context about the services generating telemetry data:</p> <p><code>service.name</code>: This is the logical name of the service. It should be the same for all instances of a horizontally scaled service. For example, if you have a shopping cart service, you might name it shoppingcart.</p> <p><code>service.namespace</code>: This is used to group related services together. It helps distinguish a group of services that logically belong to the same system or team. For example, you might use Shop as the namespace for all services related to an online store.</p> <p>These attributes help in organizing and identifying telemetry data, making it easier to monitor and troubleshoot services within a complex system.</p> <p>The logs originating from the OpenTelemetry SDK contain both the <code>service.name</code> and <code>service.namespace</code>.  However, the Pod logs which contain stdout and stderr messages from the containers - do not.  In order to make it easier to analyze the log files and unify the telemetry, the <code>service.name</code> and <code>service.namespace</code> attributes should be added to the Pod logs with Dynatrace OpenPipeline.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-name","title":"OpenTelemetry Service Name","text":"<p>Query the <code>astronomy-shop</code> logs filtered on <code>isNull(service.name)</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\")\n| filter isNull(service.name) and isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, service.name, service.namespace\n</code></pre></p> <p></p> <p>The value for <code>service.name</code> can be obtained from multiple different fields, but based on the application configuration - it is best to use the value from <code>app.label.component</code>.</p> <p>Use DQL to transform the logs and apply the <code>service.name</code> value.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\")\n| filter isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd service.name = app.label.component\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, service.name, service.namespace\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-namespace","title":"OpenTelemetry Service Namespace","text":"<p>Query the <code>astronomy-shop</code> logs filtered on <code>isNull(service.namespace)</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter isNull(service.namespace) and isNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd service.name = app.label.component\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, app.annotation.service.namespace, service.name, service.namespace\n</code></pre></p> <p></p> <p>The Pods have been annotated with the service namespace.  The <code>k8sattributes</code> processor has been configured to add this annotation as an attribute, called <code>app.annotation.service.namespace</code>.  This field can be used to populate the <code>service.namespace</code>.</p> <p>Use DQL to transform the logs and apply the <code>service.namespace</code> value.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter isNull(service.namespace) and isNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd service.name = app.label.component\n| fieldsAdd service.namespace = app.annotation.service.namespace\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, app.annotation.service.namespace, service.name, service.namespace\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-sdk-logs","title":"OpenTelemetry SDK Logs","text":"<p>The logs generated and exported by the OpenTelemetry SDK are missing Kubernetes attributes, or in some cases have the wrong values set.  The OpenTelemetry SDK, unless specifically configured otherwise, is not aware of the Kubernetes context in which of the application runs.  As a result, when the OpenTelemetry Collector that's embedded in <code>astronomy-shop</code> sends the logs to the Dynatrace OpenTelemetry Collector via OTLP, the Kubernetes attributes are populated with the Kubernetes context of the <code>astronomy-shop-otelcol</code> workload.  This makes these attributes unreliable when analyzing logs.  In order to make it easier to analyze the log files and unify the telemetry, the Kubernetes attributes should be correct for the SDK logs with Dynatrace OpenPipeline.</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language</code> and <code>astronomy-shop-otelcol</code>.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter isNotNull(telemetry.sdk.language) and matchesValue(k8s.deployment.name,\"astronomy-shop-otelcol\") and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, service.name, telemetry.sdk.language, k8s.namespace.name, k8s.deployment.name, k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n</code></pre></p> <p></p> <p>Potentially Resolved</p> <p>It is possible that in your environment this mapping of log data to the <code>astronomy-shop-otelcol</code> deployment is resolved.  If that is the case, then the query will not return any results.  That is OK.  We will still build out the OpenPipeline solution.</p> <p>The <code>k8s.namespace.name</code> is correct, however the <code>k8s.deployment.name</code>, <code>k8s.pod.name</code>, <code>k8s.pod.uid</code>, <code>k8s.replicaset.name</code>, and <code>k8s.node.name</code> are incorrect.  Since the <code>k8s.deployment.name</code> is based on the <code>service.name</code>, this field can be used to correct the <code>k8s.deployment.name</code> value.  The other values can be set to <code>null</code> in order to avoid confusion with the <code>astronomy-shop-otelcol</code> workload.</p> <p>Use DQL to transform the logs and set the <code>k8s.deployment.name</code> value while clearing the other fields.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter isNotNull(telemetry.sdk.language) and matchesValue(k8s.deployment.name,\"astronomy-shop-otelcol\") and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| sort timestamp desc\n| limit 25\n| fieldsAdd k8s.deployment.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd k8s.container.name = service.name\n| fieldsAdd app.label.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd app.label.component = service.name\n| fieldsRemove k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n| fields timestamp, service.name, telemetry.sdk.language, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.name, app.label.component\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#java-technology-bundle","title":"Java Technology Bundle","text":"<p>Many applications written in a specific programming language will utilize known logging frameworks that have standard patterns, fields, and syntax for log messages.  For Java, these include frameworks such as Log4j, Logback, java.util.logging, etc.  Dynatrace OpenPipeline has a wide variety of Technology Processor Bundles, which can be easily added to a Pipeline to help format, clean up, and optimize logs for analysis.</p> <p>The Java technology processor bundle can be applied to the <code>astronomy-shop</code> logs that we know are originating from Java applications.</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language</code> or the <code>astronomy-shop-adservice</code> Java app.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter (matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false) or matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false)) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filter matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false) or matchesValue(k8s.deployment.name,\"astronomy-shop-kafka\", caseSensitive:false)\n| sort timestamp desc\n| limit 15\n| append [fetch logs\n          | filter matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false)\n          | fieldsAdd k8s.deployment.name = concat(k8s.namespace.name,\"-\",service.name)\n          | sort timestamp desc\n          | limit 15]\n| sort timestamp desc\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, telemetry.sdk.language, content\n</code></pre></p> <p></p> <p>These are the logs that will be modified using the Java technology processor bundle within OpenPipeline.  We'll validate the results after OpenPipeline, later.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transactions","title":"PaymentService Transactions","text":"<p>Most (if not all) applications and microservices drive business processes and outcomes.  Details about the execution of these business processes is often written out to the logs by the application.  Dynatrace OpenPipeline is able to extract this business-relevant information as a business event (bizevent).</p> <p>Log to Business Event Documentation</p> <p>DQL is fast and powerful, allowing us to query log files and summarize the data to generate timeseries for dashboards, alerts, AI-driven forecasting and more.  While it's handy to generate timeseries metric data from logs when we didn't know we would need it, it's better to generate timeseries metric data from logs at ingest for the use cases that we know ahead of time.  Dynatrace OpenPipeline is able to extract metric data from logs on ingest.</p> <p>Log to Metric Documentation</p> <p>The <code>paymentservice</code> component of <code>astronomy-shop</code> generates a log record every time it processes a payment transaction successfully.  This information is nested within a <code>JSON</code> structured log record, including the transactionId, amount, cardType, and currencyCode.  By parsing these relevant logs for the fields we need, Dynatrace OpenPipeline can be used to generate a payment transaction business event and a payment transaction amount metric on log record ingest.</p> <p>Query the <code>astronomy-shop</code> logs filtered on the <code>paymentservice</code> logs with a <code>trace_id</code> attribute.</p> <p>DQL: Before OpenPipeline and DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, k8s.container.name, trace_id\n</code></pre></p> <p></p> <p>The <code>content</code> field is structured <code>JSON</code>.  The parse command can be used to parse the JSON content and add the fields we need for our use case.</p> <pre><code>{\n  \"level\": \"30\",\n  \"time\": \"1742928663142\",\n  \"pid\": \"24\",\n  \"hostname\": \"astronomy-shop-paymentservice-6fb4c9ff9b-t45xn\",\n  \"trace_id\": \"f3c6358fe776c7053d0fd2dab7bc470f\",\n  \"span_id\": \"880430306f41a648\",\n  \"trace_flags\": \"01\",\n  \"transactionId\": \"c54b6b4c-ebf1-4191-af21-5f583d0d0c87\",\n  \"cardType\": \"visa\",\n  \"lastFourDigits\": \"5647\",\n  \"amount\": {\n    \"units\": {\n      \"low\": \"37548\",\n      \"high\": \"0\",\n      \"unsigned\": false\n    },\n    \"nanos\": \"749999995\",\n    \"currencyCode\": \"USD\"\n  },\n  \"msg\": \"Transaction complete.\"\n}\n</code></pre> <p>Use DQL to transform the logs and parse the payment fields from the JSON content.</p> <p>DQL: After DQL Transformation <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, k8s.container.name, trace_id\n| parse content, \"JSON:json_content\"\n| fieldsAdd app.payment.msg = json_content[`msg`]\n| filter app.payment.msg == \"Transaction complete.\"\n| fieldsAdd app.payment.cardType = json_content[`cardType`]\n| fieldsAdd app.payment.amount = json_content[`amount`][`units`][`low`]\n| fieldsAdd app.payment.currencyCode = json_content[`amount`][`currencyCode`]\n| fieldsAdd app.payment.transactionId = json_content[`transactionId`]\n| fieldsRemove json_content\n</code></pre></p> <p></p> <p>This modifies the log attributes at query time and helps us identify the processing rules for Dynatrace OpenPipeline.  We'll validate the results after OpenPipeline, next.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#configure-openpipeline","title":"Configure OpenPipeline","text":"<p>Configure Dynatrace OpenPipeline for Astronomy Shop logs.</p> <p>View Images</p> <p>If the images are too small and the text is difficult to read, right-click and open the image in a new tab.</p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p> <p>In your Dynatrace tenant, launch the (new) <code>Settings</code> app.  From the <code>Process and contextualize</code> menu, click on <code>OpenPipeline</code>.</p> <p></p> <p>OpenPipeline App</p> <p>Depending on your Dynatrace tenant version, you may need to use the OpenPipeline app instead.</p> <p>Begin by selecting <code>Logs</code> from the menu of telemetry types.  Then choose <code>Pipelines</code>.  Click on <code>+ Pipeline</code> to add a new pipeline.</p> <p></p> <p>Name the new pipeline, <code>Astronomy Shop OpenTelemetry Logs</code>.  Click on the <code>Processing</code> tab to begin adding <code>Processor</code> rules.</p> <p></p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-name_1","title":"OpenTelemetry Service Name","text":"<p>Add a processor to set the OpenTelemetry Service Name.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry Service Name\n</code></pre></p> <p>Matching condition: <pre><code>isNull(service.name) and isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd service.name = app.label.component\n</code></pre></p> <p></p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-service-namespace_1","title":"OpenTelemetry Service Namespace","text":"<p>Add a processor to set the OpenTelemetry Service Namespace.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry Service Namespace\n</code></pre></p> <p>Matching condition: <pre><code>isNull(service.namespace) and isNotNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd service.namespace = app.annotation.service.namespace\n</code></pre></p> <p></p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#opentelemetry-sdk-logs_1","title":"OpenTelemetry SDK Logs","text":"<p>Add a processor to transform the OpenTelemetry SDK Logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>OpenTelemetry SDK Logs\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(telemetry.sdk.language) and matchesValue(k8s.deployment.name,\"astronomy-shop-otelcol\") and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p>Processor definition: <pre><code>fieldsAdd k8s.deployment.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd k8s.container.name = service.name\n| fieldsAdd app.label.name = concat(\"astronomy-shop-\",service.name)\n| fieldsAdd app.label.component = service.name\n| fieldsRemove k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n</code></pre></p> <p></p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#java-technology-bundle_1","title":"Java Technology Bundle","text":"<p>Add a processor to enrich the Java logs using the Java Technology Bundle.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Technology Bundle &gt; Java\n</code></pre></p> <p>Matching condition: <pre><code>(matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false) or matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false)) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n</code></pre></p> <p></p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transactions_1","title":"PaymentService Transactions","text":"<p>Add a processor to parse the PaymentService Transaction logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Name: <pre><code>PaymentService Transactions\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(service.name,\"paymentservice\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n</code></pre></p> <p>Processor definition: <pre><code>parse content, \"JSON:json_content\"\n| fieldsAdd app.payment.msg = json_content[`msg`]\n| fieldsAdd app.payment.cardType = json_content[`cardType`]\n| fieldsAdd app.payment.amount = json_content[`amount`][`units`][`low`]\n| fieldsAdd app.payment.currencyCode = json_content[`amount`][`currencyCode`]\n| fieldsAdd app.payment.transactionId = json_content[`transactionId`]\n| fieldsRemove json_content\n</code></pre></p> <p></p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transaction-bizevent","title":"PaymentService Transaction BizEvent","text":"<p>Switch to the <code>Data extraction</code> tab.</p> <p>Add a processor to extract a <code>Business Event</code>.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Business Event\n</code></pre></p> <p>Name: <pre><code>PaymentService Transaction\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(app.payment.cardType) and isNotNull(app.payment.amount) and isNotNull(app.payment.currencyCode) and isNotNull(app.payment.transactionId)\n</code></pre></p> <p>Event type: <pre><code>Static String : astronomy-shop.app.payment.complete\n</code></pre></p> <p>Event provider: <pre><code>Static String: astronomy-shop.opentelemetry\n</code></pre></p> <p>Field Extraction:</p> Fields app.payment.msg app.payment.cardType app.payment.amount app.payment.currencyCode app.payment.transactionId log.file.path trace_id span_id service.namespace service.name k8s.namespace.name k8s.container.name k8s.pod.name k8s.pod.uid <p></p> <p>Save Often</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#paymentservice-transaction-metric","title":"PaymentService Transaction Metric","text":"<p>Switch to the <code>Metric Extraction</code> tab.</p> <p>Add a processor to set extract a metric from the PaymentService Transaction logs.  Click on <code>+ Processor</code> to add a new processor.</p> <p>Type: <pre><code>Value metric\n</code></pre></p> <p>Name: <pre><code>PaymentService Transaction\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(app.payment.cardType) and isNotNull(app.payment.amount) and isNotNull(app.payment.currencyCode) and isNotNull(app.payment.transactionId)\n</code></pre></p> <p>Field extraction: <pre><code>app.payment.amount\n</code></pre></p> <p>Metric key: <pre><code>otel.astronomy-shop.app.payment.amount\n</code></pre></p> <p>Dimensions:</p> Field Dimension app.payment.cardType cardType app.payment.currencyCode currencyCode <p></p> <p>Consider Saving</p> <p>Consider saving your pipeline configuration often to avoid losing any changes.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#storage","title":"Storage","text":"<p>Switch to the <code>Storage</code> tab.</p> <p>Add a processor to set the bucket assignment.  Click on <code>+ Processor</code> to add a new  Bucket Assignment processor.</p> <p>Name: <pre><code>Observe and Troubleshoot Apps Bucket\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(status,\"INFO\") or matchesValue(status,\"WARN\") or matchesValue(status,\"ERROR\")\n</code></pre></p> <p>Storage: <pre><code>Observe and Troubleshoot Apps (95 Days)\n</code></pre></p> <p></p> <p>This will result in Astronomy Shop logs with <code>INFO</code>, <code>WARN</code>, or <code>ERROR</code> status values and matching this pipeline to be stored for 95 days in this bucket.</p> <p>Add a processor to set the bucket assignment.  Click on <code>+ Processor</code> to add a new  Bucket Assignment processor.</p> <p>Name: <pre><code>Log Management and Analytics Bucket\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(status,\"NONE\")\n</code></pre></p> <p>Storage: <pre><code>Log Management and Analytics (7 Days)\n</code></pre></p> <p></p> <p>This will result in Astronomy Shop logs with <code>NONE</code> status values and matching this pipeline to be stored for 7 days in this bucket.</p> <p>The pipeline is now configured, click on <code>Save</code> to save the pipeline configuration.</p> <p></p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#dynamic-route","title":"Dynamic Route","text":"<p>A pipeline will not have any effect unless logs are configured to be routed to the pipeline.  With dynamic routing, data is routed based on a matching condition. The matching condition is a DQL query that defines the data set you want to route.</p> <p>Click on <code>Dynamic Routing</code> to configure a route to the target pipeline.  Click on <code>+ Dynamic Route</code> to add a new route.</p> <p></p> <p>Configure the <code>Dynamic Route</code> to use the <code>Astronomy Shop OpenTelemetry Logs</code> pipeline.</p> <p>Name: <pre><code>Astronomy Shop OpenTelemetry Logs\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\") and isNull(event.domain)\n</code></pre></p> <p>Pipeline: <pre><code>Astronomy Shop OpenTelemetry Logs\n</code></pre></p> <p>Click <code>Add</code> to add the route.</p> <p></p> <p>Validate that the route is enabled in the <code>Status</code> column.  Click on <code>Save</code> to save the dynamic route table configuration.</p> <p></p> <p>Allow <code>astronomy-shop</code> to generate new log data that will be routed through the new pipeline (3-5 minutes).</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#analyze-results","title":"Analyze Results","text":"<p>Analyze the Astronomy Shop logs after Dynatrace OpenPipeline processing.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#analyze-the-results-in-dynatrace-notebook","title":"Analyze the results in Dynatrace (Notebook)","text":"<p>Use the Notebook from earlier to analyze the results.</p> <p>OpenTelemetry Service Name</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>isNotNull(service.name)</code> to analyze with <code>OpenTelemetry Service Name</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and matchesValue(telemetry.sdk.name,\"opentelemetry\")\n| filter isNotNull(service.name) and isNotNull(app.label.component) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, service.name, service.namespace\n</code></pre></p> <p></p> <p>OpenTelemetry Service Namespace</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>isNotNull(service.namespace)</code> to analyze with <code>OpenTelemetry Service Namespace</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter isNotNull(service.namespace) and isNotNull(service.name) and isNotNull(app.annotation.service.namespace) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.component, app.annotation.service.namespace, service.name, service.namespace\n</code></pre></p> <p></p> <p>OpenTelemetry SDK Logs</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language</code> to analyze with <code>OpenTelemetry SDK Logs</code>.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter isNotNull(telemetry.sdk.language) and isNotNull(service.name) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| sort timestamp desc\n| limit 25\n| fields timestamp, service.name, telemetry.sdk.language, k8s.namespace.name, k8s.deployment.name, k8s.container.name, app.label.name, app.label.component, k8s.pod.name, k8s.pod.uid, k8s.replicaset.name, k8s.node.name\n</code></pre></p> <p></p> <p>Java Technology Bundle</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>telemetry.sdk.language == \"java\"</code> to analyze with <code>Java Technology Bundle</code> logs.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter (matchesValue(telemetry.sdk.language,\"java\", caseSensitive: false) or matchesValue(k8s.deployment.name,\"astronomy-shop-adservice\", caseSensitive:false)) and matchesValue(k8s.namespace.name,\"astronomy-shop\")\n| filterOut matchesValue(k8s.container.name,\"istio-proxy\")\n| sort timestamp desc\n| limit 50\n</code></pre></p> <p></p> <p>You likely won't notice anything different about these logs.  This exercise was meant to show you how to use the technology bundles.</p> <p>PaymentService Transactions</p> <p>Query the <code>astronomy-shop</code> logs filtered on <code>service.name == \"paymentservice\"</code> to analyze with <code>PaymentService</code> logs.</p> <p>DQL: After OpenPipeline <pre><code>fetch logs\n| filter matchesValue(k8s.namespace.name,\"astronomy-shop\") and isNotNull(service.name)\n| filterOut event.domain == \"k8s\"\n| filter matchesValue(service.name,\"paymentservice\") and matchesValue(k8s.container.name,\"paymentservice\") and isNotNull(trace_id)\n| sort timestamp desc\n| limit 25\n| fields timestamp, content, k8s.container.name, trace_id, app.payment.msg, app.payment.cardType, app.payment.amount, app.payment.currencyCode, app.payment.transactionId\n</code></pre></p> <p></p> <p>Query the <code>PaymentService</code> Business Events.</p> <p>DQL: PaymentService Transaction Business Events <pre><code>fetch bizevents\n| filter matchesValue(event.type,\"astronomy-shop.app.payment.complete\")\n| sort timestamp desc\n| limit 10\n</code></pre></p> <p></p> <p>Query the <code>PaymentService</code> Metric.</p> <p>DQL: PaymentService Transaction Extracted Metric <pre><code>timeseries sum(`otel.astronomy-shop.app.payment.amount`), by: { currencyCode, cardType }\n| fieldsAdd value.A = arrayAvg(`sum(\\`otel.astronomy-shop.app.payment.amount\\`)`)\n</code></pre></p> <p></p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#business-data-in-context","title":"Business Data in Context","text":"<p>Extracting business data from logs has many benefits.  By generating a bizevent with OpenPipeline, the business data becomes immediately accessible for analysis, forensics, trending, and alerting - business observability.  Business events can be configured with retention times exceeding what you might choose for log retention.  Most importantly, bizevents enable business data to be captured in full context of topology, applications, infrastructure, and observability signals.  Let's explore the value of business data in context.</p> <p>Business Data</p> <p>In this example, the payment transaction details are captured on the extracted bizevent.  We can query and analyze the business data to measure business KPI and business outcomes.</p> <p></p> <p>Business Data in Kubernetes Context</p> <p>By including the Kubernetes attributes on the bizevent, this allows us to understand the topology and infrastructure that drives the business outcomes related to these payments.  Any anomalies that may occur impacting business KPIs or infrastructure health will be directly correlated, in context.</p> <p></p> <p>Business Data in Log Context</p> <p>By including a small subset of log attributes on the bizevent, this allows us to analyze diagnostic logs surrounding the payment transaction.</p> <p></p> <p>We can quickly drill down into the logs, including the surrounding logs, for each of the payment transaction bizevents.</p> <p></p> <p>Business Data in Trace Context</p> <p>By including the distributed tracing attributes on the bizevent, this allows us to analyze the distributed traces and spans for the payment transaction.</p> <p></p> <p>We can quickly drill down into the distributed trace to see everything that happened end-to-end for the payment transaction, including upstream/downstream dependencies, input parameters, and code-level details.</p> <p></p> <p>Dynatrace enables you to understand the health of business outcomes and business processes and directly tie their health to the underlying applications and infrastructure that make them possible - even with OpenTelemetry.</p>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#wrap-up","title":"Wrap Up","text":"<p>What You Learned Today</p> <p>By completing this module, you've successfully set up Dynatrace OpenPipeline pipelines to process the Astronomy Shop logs at ingest.</p> <ul> <li>Astronomy Shop logs<ul> <li>Add OpenTelemetry service name and namespace fields to unify telemetry signals and enable out-of-the-box analysis</li> <li>Enrich SDK logs with additional Kubernetes metadata to unify telemetry signals and analyze Kubernetes context</li> <li>Apply Dynatrace technology bundle (Java) to transform logs based on known Java standards and frameworks</li> <li>Extract data: Payment transaction business event to measure business outcomes and link them to system health</li> <li>Extract metrics: Payment transaction amount to measure business KPIs and link them to system health</li> <li>Routed logs to a specific bucket in Grail based on retention period needs</li> <li>Analyzed business data in context of topology, applications, infrastructure, and observability signals</li> </ul> </li> </ul>"},{"location":"5-dynatrace-openpipeline-astronomy-shop-logs/#continue","title":"Continue","text":"<ul> <li>Continue to Cleanup</li> </ul>"},{"location":"cleanup/","title":"6. Cleanup","text":"<p>Deleting the codespace from inside the container</p> <p>We like to make your life easier, for convenience there is a function loaded in the shell of the Codespace for deleting the codespace, just type <code>deleteCodespace</code>. This will trigger the deletion of the codespace.</p> <p>Another way to do this is by going to https://github.com/codespaces and delete the codespace.</p> <p>You may also want to deactivate or delete the API token needed for this lab.</p>"},{"location":"snippets/admonitions/","title":"Admonitions","text":"<p>Warning</p> <p>This is a Warning </p> <p>Note</p> <p>This is a Note </p> <p>Important</p> <p>This is important </p> <p>Tipp</p> <p>This is a tipp </p>"},{"location":"snippets/disclaimer/","title":"Disclaimer","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"snippets/grail-requirements/","title":"Grail requirements","text":"<p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"snippets/view-code/","title":"View code","text":"<p>View the Code</p> <p>The code for this repository is hosted on GitHub. Click the \"View Code on GitHub\" link above.</p>"}]}